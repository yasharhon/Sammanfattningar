\section{General Philosophy of Science}

\paragraph{Lexical Definitions}
A lexical definition is a definition based on a common understanding of a word or phrase.

\paragraph{Stipulative Definitions}
A stipulative definition is a definition based on reasons and arguments for choosing it in a particular way.

\paragraph{Knowledge}
Knowledge is true, justified belief. These three conditions are necessary, but not always sufficient. We will nevertheless proceed with this definition.

This definition is compatible with uncertainty, and recognizing uncertain knowledge is in fact vital in science. Naturally there are degrees of uncertainty.

In science, the justification of belief is central in the social aspects of its practice. An ideally justified belief requires consideration of all relevant reasons for that belief, and science is publically defended exactly to adhere to this ideal.

\paragraph{Definitions Pertaining to Phenomena}
A phenomenon $X$ is
\begin{itemize}
	\item predicted if one can know at what time $X$ will occur.
	\item explained if the causes of $X$ are known.
	\item designable if it is known to satisfy certain properties.
\end{itemize}

\paragraph{Instrumentalism and Realism}
Do scientific theories constitute claims that are true or false? Realists say yes, whereas instrumentalists say that theory only orders observations and do not in themselves contain truth.

\paragraph{Inference}
Inference is the logical process of going from a set of facts and/or evidence to some conclusion.

\paragraph{Inductive and Deductive Reasoning}
Inductive reasoning is the process of going from a set of observations to a general claim. Such reasoning is an attempt at extending our knowledge beyond what can be directly observed. Deductive reasoning is the process of explicating or extracting knowledge about something particular from previously possessed knowledge.

Inductive reasoning is fallible - even the best of efforts at induction might still overlook something for which one has no evidence. Deductive reasoning, on the other hand, is infallible - deduction necessitates that you have sufficient knowledge, and proper reasoning based on sufficient knowledge preserves the truth of your knowledge.

\paragraph{Examples of Inference}
Some examples of ways to infer follow. A few ways of inductive inference are
\begin{itemize}
	\item Direct inference, where a property is extrapolated from a finite set of observations.
	\item Projection, where a sequence or trend is used to extrapolate a future observation.
	\item Generalization, where a hypothesis is formed based on a finite set of observations.
\end{itemize}

A few ways of deductive inference are
\begin{itemize}
	\item Modus ponens, in which a set of assumptions $A$ is combined with the idea that if $A$ is true then their implications $B$ are true to infer that $B$ is true.
	\item Modus tollens, in which a conditional $A\to B$ is combined with the observation that $B$ is false to infer that $A$ is false. 
\end{itemize}

\paragraph{Hume's Problem}
As there exists an infinite set of inference rules, in particular inductive inference rules, hence the conclusions of any particular inference are only justified if the choice of inference rule is justified. Hume's problem discusses the justification of inference rules.

Hume's argument was the following:
\begin{enumerate}
	\item Assume that inference is either deductive or inductive.
	\item Assume that in order to justify an inference rule, it has to be inferred from some set of principles.
	\item Assume that inference rules cannot be deduced, as past and previous inference need not be connected,
	\item Thus, inference rules are arrived at inductively.
\end{enumerate}
The implication of the latter statement is that there must be a set of inference rules allowing you to infer the validity of the rule you want use. But this set of inference rules is not necessarily valid in itself. The result is a so-called infinite regress of the same problem of justification, which according to Hume could only be resolved if no inference rule is justified.

\paragraph{Solving Hume's Problem}
Hume's problem would imply that the practice of science is itself irrational. This would be a complete disaster, and must therefore be resolved.

One way to solve this came from Karl Popper, who denied the premise that science rests on using inference rules. This will be discussed later.

Another solution is to deny the impact of Hume's argument by studying what offers justification, in the hope that this is not destroyed by the infinite regress. Foundationalism asserts that there exists a foundation upon which justification is built. Coherentism is an alternative, which desires for claims to be justified from some coherent system containing the set of previously accepted claims. In this view, inference rules are good if they are coherent with the rest of scientific knowledge. The result is that inference rules are merely manifestations of coherence and a way to connect different practices in a coherent manner.

\paragraph{The Hypothetico-Deductive Method}
The hypothetico-deductive method consists of
\begin{enumerate}
	\item Formulating a hypothesis.
	\item Deducing consequences of the hypothesis.
	\item Test whether the consequences are true.
	\item If the consequences are false, reject the hypothesis deductively. Otherwise, strengthen your confidence in the hypothesis inductively.
\end{enumerate}

There are some requirements on the involved parts. A good hypothesis must be
\begin{itemize}
	\item Either true or false.
	\item Not a tautology, i.e. something that is necessarily true or false by definition.
	\item Contain some generalization or discussion of the unobservable.
\end{itemize}

A good set of consequences must be
\begin{itemize}
	\item Observable.
	\item Follow from valid deduction.
	\item Relevant for the hypothesis. This part informs what kinds of experiment you want to do. For instance, if the hypothesis is $A$ implies $B$, then you would need to find cases where $A$ is true and $B$ is false at the same time to reject the hypothesis. In other words, studying cases where $A$ is false or $B$ is true does not help.
\end{itemize}

\paragraph{Falsifiability and Falsification}
A theory or hypothesis is falsifiable if it has implications the truth or falsehood of which can be observed. The act of observing an implication to be false is called falsification.

\paragraph{Popper's Falsificationism}
Popper attempted to circumvent Hume's problem by rejecting the ability of science to strengthen hypotheses, leaving it only with the ability to deductively falsify them. While this seems to solve the problem, some critiques of this solution are:
\begin{itemize}
	\item It provides no way to give hypotheses confidence, in contrast to what seems reasonable and what is scientific practice.
	\item Along with an observation comes auxillary hypotheses about the validity of the experiment. A particular falsification can only falsify the conjuction of the fundamental hypothesis and the auxillary hypotheses, limiting our ability to perform modus tollens on the fundamental hypothesis. This is called the Duhem-Quine thesis.
	\item Hypotheses may be modified ad hoc solely to fit particular observations. Popper countered this by claiming that modifications that reduce falsifiability are ad hoc and should not be used, in contrast to other modifications, which might be necessary.
\end{itemize}

\paragraph{Confidence}
How can our confidence in a hypothesis be strengthened, and why does observations of consequences of a hypothesis increase our confidence in the hypothesis itself?

One answer might be found in the compatibility of the hypothesis and its consequences. However, relevance is also needed - otherwise, many observations might seemingly verify far-reaching conclusions. The issue of underdetermination is also prominent, as multiple hypotheses might be compatible with the same observations.

A better requirement is that the conclusions be unlikely if the hypothesis is false. Eliminating hypotheses based on this is called severe testing. While this is an improvement, one must first of all avoid double-counting evidence - if you use some piece of evidence to construct a hypothesis, you cannot use that evidence to verify your hypothesis. In addition, if there is low confidence in the hypothesis to begin with, severe testing will not sufficiently strengthen it.

\paragraph{Frequentism}
The frequentist view of probability is that it is the frequency with which some event occurs. As hypotheses are not events, it follows from this view that probabilities cannot be meaningfully assigned to hypotheses, limiting the possibility of quantitatively strengthening hypotheses.

\paragraph{Observation}
Loosely speaking, observation is the event of experience through the senses. It is key to all scientific practice. Observations can be divided into categories:
\begin{itemize}
	\item Direct observation, which is unaided sensory experience.
	\item Aided direct observation, which uses instruments to amplify the senses.
	\item Indirect observation, in which an event is not directly observed and the event is experienced through its effect on its surroundings.
\end{itemize}

\paragraph{Empiricism}
Empiricism is the thesis that sensory knowledge is the ultimate basis for knowledge.

\paragraph{Logical Empiricism and its Refutation}
Logical empiricism is the position that theory and experiment can be separated, that is that theory does not inform experiment. If this were true, then any scientific theory would rest on a solid and independent foundation of experimental observation.

This position has been refuted. Its refutation is as follows: First of all, indirect observation always depends on theory, as theory dictates how to interpret how the indirect observations pertain to the underlying phenomena. Next, aided direct observation sometimes requires theory as instruments may distort the phenomena they are used to observe, and separating between instrumental effects and actual phenomena requires theory. Thus theory and observation cannot be separated.

\paragraph{The Problem of Nomic Measurement}
The circular relation between theory and observation is formulated in the problem of nomic measurement. Its statement is as follows:

\begin{itemize}
	\item Assume that you want to measure property $X$, which is not directly observable.
	\item Assume that $X$ may be inferred from $Y$, which is directly observable.
	\item For the inference to be performed, one needs some relation $X = f(Y)$. This relation cannot be obtained experimentally obtained as it would require $X$ and $Y$ to be known at the same time, violating the assumption.
	\item As $f$ cannot be identified from measurement alone, it must come from theory. Thus theory and experiment are inextricably connected.
\end{itemize}

\paragraph{Operationalization}
Direct observation, while powerful in a sense, is generally error-prone and can often only give qualitative descriptions, hence indirect observational methods are usually preferred. The process of linking properties to directly observable effects is called operationalization of that property. The two are linked by some stable relation which guarantees that when the property is present, so is the observed effect. Using operationalization we may infer information about the property from observing the effect.

The quality criteria for an operationalization are
\begin{itemize}
	\item That the underlying property be well-defined in order to allow for proper inference.
	\item That the relation between the property and the effect is valid.
	\item That the relation is sufficiently stable.
	\item That the effect is publically observable with sufficient precision.
\end{itemize}

\paragraph{Operationalism}
Observationalism is the position that everything is defined through the operations by which  we observe or measure them. This is generally not a very helpful position for observationalization.

The criticisms of observationalism are
\begin{itemize}
	\item Observationalism prohibits two differently performed measurements to pertain to the same underlying concept.
	\item Observationalism makes it impossible to criticize a measurement for not properly capturing an underlying concept.
\end{itemize}

\paragraph{Measurement}
The process of measurement consists of the following:
\begin{enumerate}
	\item Define the concept that you would like to measure.
	\item Operationalize the concept.
	\item Specify a measure and define units of comparison.
	\item Represent the results with numbers.
\end{enumerate}

The requirements for a good measurement are:
\begin{itemize}
	\item There must be a unit of comparison.
	\item The unit must be sufficiently stable.
	\item Anyone must have access to the same measure.
\end{itemize}

\paragraph{Scales}
The representation part of a measurement implies the need for scale. There are five kinds of scales:
\begin{enumerate}
	\item Nominal scales, in which samples are given ID numbers.
	\item Ordinal scales, in which objects are ordered according to some qualitative measure.
	\item Interval scales, which allow the comparison of distances.
	\item Ratio scales, which allow the comparison of both distances and ratios.
	\item Absolute scales, which allow the comparison of absolute numbers, i.e. where the absolute numbers have a significance by themselves.
\end{enumerate}

Different scales of the same type made to represent the same property must represent the same empirical structure. Each set of scales is defined by the set of allowed transformations, and combined with the previous this implies that what can be inferred from a measurement is limited by the allowed transformations. The allowed transformations are:
\begin{itemize}
	\item Transformations that preserve uniqueness for nominal scales.
	\item Positive monotone transformations for ordinal scales.
	\item Positive linear transformations for interval scales.
	\item Positive scalar transformations for ratio scales.
	\item None for absolute scales.
\end{itemize}

\paragraph{Measurement Error}
The measurement error is defined as the difference between the measured and true value of some property. It comes in two forms: Systematic and random error.

Before introducing the two, we introduce the notions of precision and accuracy. Precision is the absence of variation, and accuracy is closeness to the true value. Systematic errors are thus inaccuracies, and random errors are imprecision.

\paragraph{Reducing Error}
Random error can be reduced by repeating measurements and performing new operationalization in such a way that you obtain more precise measurements.

Before discussing reducing systematic error, we introduce convergent and divergent validity. The former is that different ways to measure the same intended property should yield the same result, and the latter is that different ways to measure different intended properties should yield different results.

Now, to reduce systematic error a first approach is to identify the causes. The convergent and divergent validity of your results is one way to look. You can also work to reduce observer effects, as well as stabilize and standardize your measurements.

The control of error represents a set of auxillary hypotheses which enter in the hypothetico-deductive method.