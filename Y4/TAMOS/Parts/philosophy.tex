\section{General Philosophy of Science}

\paragraph{Definitions}
Definitions explain the meaning of words. They consist of a definiendum (what is to be defined), a definiens (that which defines the definiendum) and a defining connective (which describes how the two are connected. Words such as ``is'' as well as logical comparisons are in this category). In addition, there may be delimiters which explain that the definiendum is an aspect of something else. For instance, the definition ``A system is spherically symmetric if it looks the same from all directions,'' ``spherically symmetric'' is the definiendum and ``A system is'' is the delimiter.

\paragraph{Lexical Definitions}
A lexical definition is a definition based on a common understanding of a word or phrase.

\paragraph{Stipulative Definitions}
A stipulative definition is a definition based on reasons and arguments for choosing it in a particular way.

\paragraph{Ambiguity}
Ambiguity is the presence of two possible meanings of the same word.

\paragraph{Vagueness}
Vagueness is the unclarity of the limits of what is within the scope of a definition.

\paragraph{Narrowness and Broadness}
A definition is too narrow if it does not apply to something that it should, and too broad if it applies to something that it should not.

\paragraph{Knowledge}
Knowledge is true, justified belief. These three conditions are necessary, but not always sufficient. We will nevertheless proceed with this definition.

This definition is compatible with uncertainty, and recognizing uncertain knowledge is in fact vital in science. Naturally there are degrees of uncertainty.

In science, the justification of belief is central in the social aspects of its practice. An ideally justified belief requires consideration of all relevant reasons for that belief, and science is publically defended exactly to adhere to this ideal.

\paragraph{Definitions Pertaining to Phenomena}
A phenomenon $X$ is
\begin{itemize}
	\item predicted if one can know at what time $X$ will occur.
	\item explained if the causes of $X$ are known.
	\item designable if it is known to satisfy certain properties.
\end{itemize}

\paragraph{Instrumentalism and Realism}
Do scientific theories constitute claims that are true or false? Realists say yes, whereas instrumentalists say that theory only orders observations and do not in themselves contain truth.

\paragraph{Inference}
Inference is the logical process of going from a set of facts and/or evidence to some conclusion.

\paragraph{Inductive and Deductive Reasoning}
Inductive reasoning is the process of going from a set of observations to a general claim. Such reasoning is an attempt at extending our knowledge beyond what can be directly observed. Deductive reasoning is the process of explicating or extracting knowledge about something particular from previously possessed knowledge.

Inductive reasoning is fallible - even the best of efforts at induction might still overlook something for which one has no evidence. Deductive reasoning, on the other hand, is infallible - deduction necessitates that you have sufficient knowledge, and proper reasoning based on sufficient knowledge preserves the truth of your knowledge.

\paragraph{Examples of Inference}
Some examples of ways to infer follow. A few ways of inductive inference are
\begin{itemize}
	\item Direct inference, where a property is extrapolated from a finite set of observations.
	\item Projection, where a sequence or trend is used to extrapolate a future observation.
	\item Generalization, where a hypothesis is formed based on a finite set of observations.
\end{itemize}

A few ways of deductive inference are
\begin{itemize}
	\item Modus ponens, in which a set of assumptions $A$ is combined with the idea that if $A$ is true then their implications $B$ are true to infer that $B$ is true.
	\item Modus tollens, in which a conditional $A\to B$ is combined with the observation that $B$ is false to infer that $A$ is false. 
\end{itemize}

\paragraph{Hume's Problem}
As there exists an infinite set of inference rules, in particular inductive inference rules, hence the conclusions of any particular inference are only justified if the choice of inference rule is justified. Hume's problem discusses the justification of inference rules.

Hume's argument was the following:
\begin{enumerate}
	\item Assume that inference is either deductive or inductive.
	\item Assume that in order to justify an inference rule, it has to be inferred from some set of principles.
	\item Assume that inference rules cannot be deduced, as past and previous inference need not be connected,
	\item Thus, inference rules are arrived at inductively.
\end{enumerate}
The implication of the latter statement is that there must be a set of inference rules allowing you to infer the validity of the rule you want use. But this set of inference rules is not necessarily valid in itself. The result is a so-called infinite regress of the same problem of justification, which according to Hume could only be resolved if no inference rule is justified.

\paragraph{Solving Hume's Problem}
Hume's problem would imply that the practice of science is itself irrational. This would be a complete disaster, and must therefore be resolved.

One way to solve this came from Karl Popper, who denied the premise that science rests on using inference rules. This will be discussed later.

Another solution is to deny the impact of Hume's argument by studying what offers justification, in the hope that this is not destroyed by the infinite regress. Foundationalism asserts that there exists a foundation upon which justification is built. Coherentism is an alternative, which desires for claims to be justified from some coherent system containing the set of previously accepted claims. In this view, inference rules are good if they are coherent with the rest of scientific knowledge. The result is that inference rules are merely manifestations of coherence and a way to connect different practices in a coherent manner.

\paragraph{The Hypothetico-Deductive Method}
The hypothetico-deductive method consists of
\begin{enumerate}
	\item Formulating a hypothesis.
	\item Deducing consequences of the hypothesis.
	\item Test whether the consequences are true.
	\item If the consequences are false, reject the hypothesis deductively. Otherwise, strengthen your confidence in the hypothesis inductively.
\end{enumerate}

There are some requirements on the involved parts. A good hypothesis must be
\begin{itemize}
	\item Either true or false.
	\item Not a tautology, i.e. something that is necessarily true or false by definition.
	\item Contain some generalization or discussion of the unobservable.
\end{itemize}

A good set of consequences must be
\begin{itemize}
	\item Observable.
	\item Follow from valid deduction.
	\item Relevant for the hypothesis. This part informs what kinds of experiment you want to do. For instance, if the hypothesis is $A$ implies $B$, then you would need to find cases where $A$ is true and $B$ is false at the same time to reject the hypothesis. In other words, studying cases where $A$ is false or $B$ is true does not help.
\end{itemize}

\paragraph{Falsifiability and Falsification}
A theory or hypothesis is falsifiable if it has implications the truth or falsehood of which can be observed. The act of observing an implication to be false is called falsification.

\paragraph{Popper's Falsificationism}
Popper attempted to circumvent Hume's problem by rejecting the ability of science to strengthen hypotheses, leaving it only with the ability to deductively falsify them. While this seems to solve the problem, some critiques of this solution are:
\begin{itemize}
	\item It provides no way to give hypotheses confidence, in contrast to what seems reasonable and what is scientific practice.
	\item Along with an observation comes auxillary hypotheses about the validity of the experiment. A particular falsification can only falsify the conjuction of the fundamental hypothesis and the auxillary hypotheses, limiting our ability to perform modus tollens on the fundamental hypothesis. This is called the Duhem-Quine thesis.
	\item Hypotheses may be modified ad hoc solely to fit particular observations. Popper countered this by claiming that modifications that reduce falsifiability are ad hoc and should not be used, in contrast to other modifications, which might be necessary.
\end{itemize}

\paragraph{Confidence}
How can our confidence in a hypothesis be strengthened, and why does observations of consequences of a hypothesis increase our confidence in the hypothesis itself?

One answer might be found in the compatibility of the hypothesis and its consequences. However, relevance is also needed - otherwise, many observations might seemingly verify far-reaching conclusions. The issue of underdetermination is also prominent, as multiple hypotheses might be compatible with the same observations.

A better requirement is that the conclusions be unlikely if the hypothesis is false. Eliminating hypotheses based on this is called severe testing. While this is an improvement, one must first of all avoid double-counting evidence - if you use some piece of evidence to construct a hypothesis, you cannot use that evidence to verify your hypothesis. In addition, if there is low confidence in the hypothesis to begin with, severe testing will not sufficiently strengthen it.

\paragraph{Frequentism}
The frequentist view of probability is that it is the frequency with which some event occurs. As hypotheses are not events, it follows from this view that probabilities cannot be meaningfully assigned to hypotheses, limiting the possibility of quantitatively strengthening hypotheses.

\paragraph{Observation}
Loosely speaking, observation is the event of experience through the senses. It is key to all scientific practice. Observations can be divided into categories:
\begin{itemize}
	\item Direct observation, which is unaided sensory experience.
	\item Aided direct observation, which uses instruments to amplify the senses.
	\item Indirect observation, in which an event is not directly observed and the event is experienced through its effect on its surroundings.
\end{itemize}

\paragraph{Empiricism}
Empiricism is the thesis that sensory knowledge is the ultimate basis for knowledge.

\paragraph{Logical Empiricism and its Refutation}
Logical empiricism is the position that theory and experiment can be separated, that is that theory does not inform experiment. If this were true, then any scientific theory would rest on a solid and independent foundation of experimental observation.

This position has been refuted. Its refutation is as follows: First of all, indirect observation always depends on theory, as theory dictates how to interpret how the indirect observations pertain to the underlying phenomena. Next, aided direct observation sometimes requires theory as instruments may distort the phenomena they are used to observe, and separating between instrumental effects and actual phenomena requires theory. Thus theory and observation cannot be separated.

\paragraph{The Problem of Nomic Measurement}
The circular relation between theory and observation is formulated in the problem of nomic measurement. Its statement is as follows:

\begin{itemize}
	\item Assume that you want to measure property $X$, which is not directly observable.
	\item Assume that $X$ may be inferred from $Y$, which is directly observable.
	\item For the inference to be performed, one needs some relation $X = f(Y)$. This relation cannot be obtained experimentally obtained as it would require $X$ and $Y$ to be known at the same time, violating the assumption.
	\item As $f$ cannot be identified from measurement alone, it must come from theory. Thus theory and experiment are inextricably connected.
\end{itemize}

\paragraph{Operationalization}
Direct observation, while powerful in a sense, is generally error-prone and can often only give qualitative descriptions, hence indirect observational methods are usually preferred. The process of linking properties to directly observable effects is called operationalization of that property. The two are linked by some stable relation which guarantees that when the property is present, so is the observed effect. Using operationalization we may infer information about the property from observing the effect.

The quality criteria for an operationalization are
\begin{itemize}
	\item That the underlying property be well-defined in order to allow for proper inference.
	\item That the relation between the property and the effect is valid.
	\item That the relation is sufficiently stable.
	\item That the effect is publically observable with sufficient precision.
\end{itemize}

\paragraph{Operationalism}
Observationalism is the position that everything is defined through the operations by which  we observe or measure them. This is generally not a very helpful position for observationalization.

The criticisms of observationalism are
\begin{itemize}
	\item Observationalism prohibits two differently performed measurements to pertain to the same underlying concept.
	\item Observationalism makes it impossible to criticize a measurement for not properly capturing an underlying concept.
\end{itemize}

\paragraph{Measurement}
The process of measurement consists of the following:
\begin{enumerate}
	\item Define the concept that you would like to measure.
	\item Operationalize the concept.
	\item Specify a measure and define units of comparison.
	\item Represent the results with numbers.
\end{enumerate}

The requirements for a good measurement are:
\begin{itemize}
	\item There must be a unit of comparison.
	\item The unit must be sufficiently stable.
	\item Anyone must have access to the same measure.
\end{itemize}

\paragraph{Scales}
The representation part of a measurement implies the need for scale. There are five kinds of scales:
\begin{enumerate}
	\item Nominal scales, in which samples are given ID numbers.
	\item Ordinal scales, in which objects are ordered according to some qualitative measure.
	\item Interval scales, which allow the comparison of distances.
	\item Ratio scales, which allow the comparison of both distances and ratios.
	\item Absolute scales, which allow the comparison of absolute numbers, i.e. where the absolute numbers have a significance by themselves.
\end{enumerate}

Different scales of the same type made to represent the same property must represent the same empirical structure. Each set of scales is defined by the set of allowed transformations, and combined with the previous this implies that what can be inferred from a measurement is limited by the allowed transformations. The allowed transformations are:
\begin{itemize}
	\item Transformations that preserve uniqueness for nominal scales.
	\item Positive monotone transformations for ordinal scales.
	\item Positive linear transformations for interval scales.
	\item Positive scalar transformations for ratio scales.
	\item None for absolute scales.
\end{itemize}

\paragraph{Measurement Error}
The measurement error is defined as the difference between the measured and true value of some property. It comes in two forms: Systematic and random error.

Before introducing the two, we introduce the notions of precision and accuracy. Precision is the absence of variation, and accuracy is closeness to the true value. Systematic errors are thus inaccuracies, and random errors are imprecision.

\paragraph{Reducing Error}
Random error can be reduced by repeating measurements and performing new operationalization in such a way that you obtain more precise measurements.

Before discussing reducing systematic error, we introduce convergent and divergent validity. The former is that different ways to measure the same intended property should yield the same result, and the latter is that different ways to measure different intended properties should yield different results.

Now, to reduce systematic error a first approach is to identify the causes. The convergent and divergent validity of your results is one way to look. You can also work to reduce observer effects, as well as stabilize and standardize your measurements.

The control of error represents a set of auxillary hypotheses which enter in the hypothetico-deductive method.

\paragraph{Experiments}
An experiment is a controlled observation in which the observer manipulates the real variables that are believed to influence the outcome, both for the purpose of intervention and control. Its purpose is to justify accepting or rejecting a hypothesis. Its characteristics are:
\begin{itemize}
	\item manipulation.
	\item intervention with the independent variables.
	\item control of disturbing factors.
	\item observation.
\end{itemize}

\paragraph{Mill's Method of Difference}
Mill's method of difference describes the logic and procedure of experiments. It has the following steps:
\begin{enumerate}
	\item Ask what causes a phenomenon $E$.
	\item Conjecture that $C$ is the cause.
	\item Produce situations $S_{1}$ and $S_{2}$ in which neither $C$ nor $E$ occur and such that all relevant causal factors are the same.
	\item Activate $C$ in $S_{1}$ only.
	\item Observe $E$ in $S_{1}$ only.
	\item Assert that something causes $E$ in $S_{1}$ and that nothing causes $E$ in $S_{2}$.
	\item As the two only differ by $C$, conclude that $C$ causes $E$.
\end{enumerate}

Baked into these steps are a number of assertions and ideas which might be difficult to realize - for instance, it might be hard to know what factors are causal. This necessitates good design.

\paragraph{Randomization}
Randomization is the process of dividing samples into treatment and control groups using randomness. Experiments set up this way are called randomized controlled trials.

Randomness is desired because it might eliminate selection bias, help convince others that your experiment is not rigged and blinds the identity of the treatment from both investigators and subjects. However, randomization is not the only way to achieve these goals.

Randomization does not help to equally distribute background factors between the treatment and control groups. Imbalances may instead be avoided by being checked and controlled for. One might also use so-called stratified randomization, where background factors are included in the randomization process.

Randomization has no bearing of the broadness of your sample, meaning that it adds nothing to the generalizability of your results to other populations than that which you have sampled.

\paragraph{Repeatability, Reproducibility and Replicability}
If an experiment is described such that it contains enough information that others can repeat it, the experiment is repeatable. If a competent repetition of the experiment yields the same result, the experiment is reproducible. If a competent independent experiment using independent data, methods and experimental infrastructure yields the same results, the experiment is said to be replicable. These categories will be important when looking for errors in experimental design.

\paragraph{Design Errors}
An erroneous design may manifest as:
\begin{itemize}
	\item Failure to control for all relevant factors.
	\item Confirmation bias(/interpretation problem) in observations.
	\item The observer effect(/influence problem) spoiling your results.
	\item The placebo effect interfering.
	\item Selection bias in your treatment and control groups.
\end{itemize}
If an experiment sufficiently controls for such errors, conclusions drawn from the experiment are said to be internally valid.

To detect such errors, one might employ what knowledge one possesses, of either theory or experimental practice, to identify them, or one might investigate the repeatability, reproducibility and replicability of the experiment to find out where it might have gone wrong.

\paragraph{Experimental Control}
Experimental control consists of identifying relevant features that might interfere with your experiment and being able to influence these features such that alternative explanations can be ruled out.

A few methods for implementing experimental control are:
\begin{itemize}
	\item Dividing your subjects into treatment and control groups.
	\item Keeping other factors constant by finding or creating situations with the same background.
	\item Eliminating background factors altogether by creating special circumstances. One way to do this is to blind your study. There is single blinding, where the subjects do not know whether they are in the treatment or control group, and double blinding, where neither the subject nor the experimenter knows to which group the subject belongs.
	\item Separate out external factors by some special experimental construction.
\end{itemize}

\paragraph{Non-Experimental Procedures}
I briefly mention some examples of non-theoretical activities in science that are not experiments. The first is observational studies, where you study subjects with no ability to manipulate, intervene or control the subjects. Two others are natural experiments and field experiments, in which you have no ability to manipulate or intervene, but by construction you may still control for some factors. A final one is simulation, where you have the ability to both manipulate, intervene and control, but only on a representation of your system.

\paragraph{Models}
Models are important tools in science used to help us describe the world. They have different aspects, and may be approached through any of them. These are:
\begin{itemize}
	\item Representations, where the models attempt to represent some target in the real world to some extent. This is useful when studying the real-world target might be impossible or infeasible, morally or ethically prohibited or cognitively difficult.
	\item Idealizations, where the model contains only the relevant aspects of the real-world target. Models are generally analogies of the real-world target. These may be positive (containing similar aspects), negative (containing idealizations) or neutral (containing descriptions of things that cannot be known in the target).
	\item Purpose-dependent tools - some models describe certain properties better than others.
	\item Things to be manipulated. The kind of analogy provided by the model limits the available set of manipulations - if some property is idealized away in the model, then investigating that property in the model has no bearing on the target. Manipulations may also be used to reveal neutral analogies as either positive or negative.
\end{itemize}

\paragraph{Models and Experiments}
Models and experiments are similar in that they contain some variables, the manipulation of something and the observation of the effect. However, they differ in the type of errors that can be made. Experiments rely on internal validity, whereas this is less of a problem for models. By contrast, models have greater issues with justifying neutral analogies.

\paragraph{Quality Criteria for Models}
Models do not necessarily have quality criteria that apply equally well in all context, and in fact these often trade off with one another. Nevertheless, some criteria are:
\begin{itemize}
	\item Similarity to the target with respect to relevant properties. This may even be termed accuracy.
	\item Robustness. A model is robust with respect to some condition if changing it does not change the result.
	\item Precision. A model is more precise than another if the specification of the former imply the specification of the latter.
	\item Simplicity. A model is simpler than another if it contains fewer variables, fewer parameters and fewer observations.
	\item Tractability. A model is tractable with respect to some principles if the results can be obtained by applying the set of principles to the model.
	\item Transparency. A model is transparent if the user is capable of understanding how the results are produced.
\end{itemize}

\paragraph{Modelling Strategies}
I here introduce two modelling strategies. Their approaches reflect how we learn from models.

The first is to consider models as mirrors of the world. In this view you desire your model to be precise, a feature that you typically gain at the cost of simplicity, tractability and transparency. You will still not gain enough precision to avoid issues of external validity, i.e. to guarantee that the model describes the target.

The other view is to view models as isolations of relevant features of a system. This view requires that the target be divisible, at least to some extent, in such a way. It is also difficult to validate, as the target also has interaction effects.

\paragraph{Scientific Knowledge}
The aims of scientific knowledge are three-fold:
\begin{itemize}
	\item Prediction - providing reasons for expecting a phenomenon to occur in a particular way.
	\item Design - providing reasons for expecting why a certain manipulation satisfies certain functions.
	\item Explanation - providing reasons for why a phenomenon is based on a lawful basis.
\end{itemize}
Our description of explanation seemingly makes it very similar to prediction - one looks to the future, the other to the past. This is a weakness, and will require us to modify the above.

\paragraph{The Deductive-Nomological Account}
The deductive-nomological account is way of providing an explanation. It answers the question of why some particular phenomenon occured by showing it resulted from a set of natural laws and circumstances.

\paragraph{Understanding}
Understanding a phenomenon can be taken to be the ability to answer questions about what would happen if things were different. The process of understanding thus involves tracing relations between so-called productive relations, which describes what features of a system produces what others.

\paragraph{Critiques of the Deductive-Nomological Account}
If we modify our concept of explanation to providing understanding, we see that providing explanation must entail identifying productive relations. This is a better notion of explanation because it helps us distinguish between explanation and prediction. The deductive-nomological account does not distinguish between the producer and what is produced, and is therefore insufficient to provide explanations. Furthermore, the deductive-nomological account does not identify the relevance of the different circumstances.

Another critique comes from considering so-called singular causal explanations. These are explanations based purely on recounting a sequence of events. In many practical contexts such explanations are sufficient, but according to the deductive-nomological account they are not. Hence the deductive-nomological account does not cover everything that we would like to consider an explanation.

\paragraph{The Structure of Explanations}
Explanations consists of an explanandum - what is to explained - and one or more explanans - statements that increase understanding. An important ingredient in explanations is contrasting the explanandum to some other scenario, and this contrast might affect the answer you seek or obtain.

\paragraph{Causal Explanations and their Quality Criteria}
Causal explanations are explanations that identify difference-making contributing causes to an explanandum. Their quality criteria are:
\begin{itemize}
	\item Accuracy - the ability of the explanans to describe the state or properties. The explanans only needs to identify the difference-making contributions.
	\item Precision - the more precisely the explanandum states a contrast, the better.
	\item Difference-making - the explanans must identify all contributing causes.
	\item Non-sensitivity - the explanans must have low sensitivity to background causes.
	\item Cognitive salience - the explanation should be fit to its audience. This typically means making it simpler.
\end{itemize}

\paragraph{Causation}
We try to clarify the notion of causation by defining some associated terms.

$X$ is a direct cause of $Y$ with respect to a set of background variables $V$ if and only if there exists an intervention on $X$ that changes $Y$ with $V$ held constant. Built into this notion is the idea that $X$ causes $Y$ with respect to some background variables, which is a model. Therefore one cannot say that $X$ causes $Y$ generally.

$X$ is a contributing cause of $Y$ with respect to a set of background variables $V$ if and only if there exists a causal chain comprised of direct causes extending from $X$ to $Y$.

\paragraph{Strategies for Identifying Causality}
While correlations are not sufficient or even necessary for causal relations, they are an important part of the evidence for certain causal relations. One strategy for identifying causal relations is Mill's method of difference, which has the drawback of requiring experiments. Another is instrumental variable analysis, defined by the following steps:
\begin{itemize}
	\item Observe a correlation between $X$ and $Y$.
	\item Find a variable $Z$ that is known to affect $X$, but not $Y$.
	\item Use $Z$ instead of $X$ when estimating the effect of $X$ on $Y$.
\end{itemize}
This strategy lets you work out causal relations only by observing correlations, but requires some causal knowledge in order to be implemented.