\section{Matriser}

\subsection{Definitioner}

\paragraph{Kolonnrum}
Kolonnrummet till matrisen $A$, $\Col{A}$, är det linjära höljet av kolumnerna i $A$.

\paragraph{Radrum}
Radrummet till matrisen $A$, $\Row{A}$, är det linjära höljet av raderna i $A$.

\paragraph{Nollrum}
Nollrummet till matrisen $A$ är definierad som
\begin{align*}
	\Null{A} = \left\{\vect{x}:\ A\vect{x} = 0\right\}.
\end{align*}

\paragraph{Matris-vektor-produkt}
Betrakta $m\times n$-matrisen
\begin{align*}
	A =
	\left[\begin{array}{cccc}
    	a_{1,1} & a_{1,2} & \dots  & a_{1,n} \\
    	a_{2,1} & a_{2,2} & \dots  & a_{2,n} \\
    	\vdots  & \vdots  & \ddots & \vdots  \\
	    a_{m,1} & a_{m,2} & \dots  & a_{m,n}
	\end{array}\right]
\end{align*}
och vektoren i $\R^n$
\begin{align*}
	\vectorbold{x} =
	\left[\begin{array}{c}
    	x_1    \\
    	x_2    \\
    	\vdots \\
	    x_n
	\end{array}\right]
\end{align*}
Matrisproduktet $A\vectorbold{x}$ definieras som vektoren
\begin{align*}
	A\vectorbold{x} =
	\left[\begin{array}{c}
    	a_{1,1}x_1 + a_{1,2}x_2 + \dots + a_{1,n}x_n \\
    	a_{2,1}x_1 + a_{2,2}x_2 + \dots + a_{2,n}x_n \\
    	\vdots                                       \\
	    a_{m,1}x_1 + a_{m,2}x_2 + \dots + a_{m,n}x_n
	\end{array}\right]
\end{align*}
i $\R^m$.

\paragraph{Homogena ekvationssystem}
Ett homogent ekvationssystem kan skrivas på formen
\begin{align*}
	A\vect{x} = \vect{0}.
\end{align*}
Motsatsen kallas inhomogena ekvationssystem.

\paragraph{Addition av matriser}
För två matriser $A = (a_{i,j}), B = (b_{i,j})$ har man
\begin{align*}
	A + B = (a_{i,j} + b_{i,j}).
\end{align*} 

\paragraph{Mutliplikation av matriser med konstanter}
För en matris $A = (a_{i,j})$ har man
\begin{align*}
	cA = (ca_{i,j}), c\in\R.
\end{align*}

\paragraph{Diagonalmatriser}
En matris $D = (d_{i,j})$ kallas en diagonal matris om $d_{i,j} = 0$ när $i\neq j$.

\paragraph{Transponat}
För en matris $A = (a_{i,j})$ definieras transponatet som $A^T = (a_{j,i})$.

\paragraph{Matrismultiplikation}
Matrismultiplikation av en $m\times p$-matris $A$ och en $p\times n$-matris $B$ ges av
\begin{align*}
	AB = C: c_{i,j} = \sum\limits_{k = 1}^{p} a_{i, k}b_{k,j}.
\end{align*}

\paragraph{Inversen av en matris}
En matris $A$ sin invers $A^{-1}$ uppfyller
\begin{align*}
	AA^{-1} = A^{-1}A = I.
\end{align*}

\paragraph{Elementärmatriser}
En matris $E$ är en elementärmatris om produktet $EA$ kan fås vid att göra en radoperation på $A$.

\paragraph{Rang}
Rangen till en matris, skrivit som $\rank A$, är $\dim{\Col A}$.

\paragraph{Determinant}
För en $n\times n$-matris $A$ ges determinanten av
\begin{align*}
	\det{A} = \abs{A} &= \sum\limits_{i = 1}^{n}(-1)^{i + j}a_{ij}\det{A_{ij}} \\
	&= \sum\limits_{j = 1}^{n}(-1)^{i + j}a_{ij}\det{A_{ij}},
\end{align*}
där $A_{ij}$ är matrisen $A$ utan rad $i$ och kolumn $j$. De två summorna visar att man kan räkna ut determinanten vid att utveckla den långs en given kolumn $j$ i första fallet eller en given rad $i$ i det andra fallet. Formelen är rekursiv, och base case är $n=2$, som ges av
\begin{align*}
	\mdet{a & d \\ c & b} = ab - cd
\end{align*}
eller $n = 1$, som ges av
\begin{align*}
	\mdet{a} = a.
\end{align*}

\paragraph{Egenvektorer, egenvärden och matriser}
Egenvektorer och egenvärden definieras analogt för matriser som för linjära avbildningar. För detaljer, se \ref{par:eigen_values}.

\paragraph{Similära matriser och diagonalisering}
Två matriser $A$ och $B$ är similära om det finns en matris $P$ så att
\begin{align*}
	A = PBP^{-1}.
\end{align*}
Om $B$ är en diagonalmatris säjs $A$ vara diagonaliserbar.

\paragraph{Ortogonal diagonaliserbarhet}
$n\times n$-matrisen $A$ är ortogonalt diagonaliserbar om den är diagonaliserbar och det finns en ortonormal bas av egenvektorer. För mer om ortogonalitet, se \ref{par:orthogonality}.

\paragraph{Ortogonala matriser}
En ortogonal matris är en matris med ortonormala kolumner.

\paragraph{Markovkedjor}
En markovkedje representeras av en matris så att kolumnerna summeras till $1$. Sådana matriser kan användas t.ex. för att beskriva stokastiska processer (processer baserade på sannolikhet).

\subsection{Satser}

\paragraph{Matriskolumner och linjära höljen}
Följande påståenden är ekvivalenta:
\begin{itemize}
	\item[a)] $A\vect{x} = \vect{b}$ har lösning för varje $\vect{b}$.
	\item[b)] Varje $\vect{b}\in \R^{m}$ är en linjär kombination av kolumnerna i $A$.
	\item[c)] $\Span{\vect{A_1}, \vect{A_2}, \dots, \vect{A_n}} = \R^m$, där $\vect{A_i}$ är kolumnerna i $A$.
	\item[d)] $A$ kan reduceras till en matris med $m$ ledande ettor.
\end{itemize}

\proof
Ekvivalensen till a, b och c är vel trivial eller någonting.

Låt $U$ vara en echelonform av $A$. Då kan ekvationssystemet $A\vect{x} = \vect{b}$ sin totalmatris $[A | \vect{b}]$ radreduceras till $[U | \vect{d}]$ för något $\vect{d}$.

Om d är sann, har systemet en unik lösning, och a är även uppfyld. Om d ej är sann, låt den sista raden i $U$ endast vara nollor och låt det siste elementet i $\vect{d}$ vara $1$. Detta representerar ett inkonsekvent ekvationssystem. Radroperationerna kan reverseras till man får en totalmatris som representerar $A\vect{x} = \vect{b}$, som fortfarande är inkonsekvent. Då har systemet $A\vect{x} = \vect{b}$ ej en lösning, och a är falsk. Detta beviser ekvivalensen.

\paragraph{Lösningen till inhomogena ekvationssystem}
Om det inhomogena ekvationssystemet
\begin{align*}
	A\vect{x} = \vect{0}
\end{align*}
har lösningen $\vect{x}_\text{h}$, har det inhomogena ekvationssystemet
\begin{align*}
	A\vect{x} = \vect{b}
\end{align*}
lösningen $\vect{x} = \vect{x}_\text{h} + \vect{x}_\text{i}$, var $\vect{x}_\text{i}$ är någon vektor som uppfyllar det inhomogena ekvationssystemet.

\proof


\paragraph{Linjärt beroende av kolumner i en matris}
Kolumnerna i en matris är linjärt oberoende omm (om och endast om) $A\vect{x} = \vect{0}$ endast har den triviala lösningen. Specielt gäller det att om antal rader är mindre än antal kolumner är kolumnvektorerna linjärt beroende.

\proof

\paragraph{Inverterbarhet av en matris}
En matris $A$ är inverterbar om och endast om $\det A\neq 0$.

\proof

\paragraph{Rangsatsen}
För en $n\times m$-matris $A$ är $\rank A + \dim{\Null A} = m$.

\proof

\paragraph{Determinanten för triangulära matriser}
För en triangulär $n\times n$-matris $A$, dvs. en matris som har endast nollor över eller under diagonalen, ges determinanten av
\begin{align*}
	\det{A} = \prod\limits_{i = 1}^{n} a_{ii}.
\end{align*}

\proof

\paragraph{Determinant och elementärmatriser}
För en elementärmatris $E$ har man
\begin{align*}
	\det{E} =
	\begin{cases}
		-1, &E\text{ byter plats på två rader.} \\
		1,  &E\text{ adderar en multippel av en rad till en annan.} \\
		t,  &E\text{ multiplicerar en rad med en skalär}~t\neq 0.
	\end{cases}
\end{align*}
och att
\begin{align*}
	B = EA \implies \det{B} = \det{E}\det{A}
\end{align*}

\proof

\paragraph{Determinant för matrisprodukt}
\begin{align*}
	\det{AB} = \det{A}\det{B}
\end{align*}

\proof

\paragraph{Diagonaliserbarhet}
En $n\times n$-matris $A$ är diagonaliserbar om och endast om den har $n$ linjärt oberoende egenvektorer. Detta ger även att kolumnerna i $P$ är egenvektorerna till $A$ och $D$ är en diagonalmatris med egenvärden motsvarande kolumnerna i $P$ på diagonalen.

\proof

\paragraph{Ortogonal diagonaliserbarhet}
En matris är ortogonalt diagonaliserbar om den är symmetrisk, d.v.s. $A = A^T$, och den är diagonaliserbar.

\proof

\paragraph{Ortogonala matriser och längd}
Låt $A$ vara en ortogonal matris. Då är
\begin{align*}
	\abs{A\vect{u}} = \abs{\vect{u}}.
\end{align*}

\proof
\begin{align*}
	A\vect{u}         &= \sum\limits_{i = 1}^{n}u_i\vect{a}_i \\
	\abs{A\vect{u}}^2 &= A\vect{u}\cdot A\vect{u} = \left(\sum\limits_{i = 1}^{n}u_i\vect{a}_i\right)\cdot\left(\sum\limits_{i = 1}^{n}u_i\vect{a}_i\right) = \sum\limits_{i = 1}^{n}u_i\vect{a}_i\cdot\sum\limits_{i = 1}^{n}u_i\vect{a}_i
\end{align*}
Eftersom kolumnerna i $A$ är ortogonala blir skalärprodukterna
\begin{align*}
	\vect{u}_i\cdot\vect{u}_j = \delta_{ij} =
	\begin{cases}
		1, i = j \\
		0, i\neq j
	\end{cases}
\end{align*}
även känd som Kronecker-deltafunktionen. Detta ger
\begin{align*}
	\abs{A\vect{u}}^2 = \sum\limits_{i = 1}^{n}u_i^2 = \abs{u}_i^2.
\end{align*}

\paragraph{Spektralsatsen}
Låt $A$ vara en $n\times n$-matris. Då gäller att:
\begin{itemize}
	\item $A$ har $n$ reella egenvärden, upp till multiplicitet.
	\item Dimensionen till egenrummet motsvarande ett givet egenvärde är lika med egenvärdets multiplicitet.
	\item Egenrummen är ortogonala.
	\item $A$ är ortogonalt diagonaliserbar.
\end{itemize}

\proof