\section{Derivata}

\subsection{Definitioner}

\paragraph{Partiella derivator}
Låt $f: D\to\R^p$ med $D\subset\R^n$. $f$ är partiellt deriverbar med avseende på $x_i$ i den inre punkten $\vect{a}\in D$ om gränsvärdet
\begin{align*}
	\limit{h}{0}\frac{f(\vect{a} + h\vect{e}_i) - f(\vect{a})}{h}
\end{align*}
existerar. Gränsvärdet kallas partiella derivatan av $f$ med avseende på $x_i$ i $\vect{a}$ och betecknas $\pdeval{f}{x_i}{\vect{a}}$.

\paragraph{Differentierbarhet}
Låt $f: D\to\R$ med $D\subset\R^n$. $f$ är differentierbar i $\vect{a}$ om $\exists A_1, \dots, A_n$ och en $\rho(\vect{h})$ så att
\begin{align*}
	f(\vect{a} + \vect{h}) - f(\vect{a}) = \sum\limits_{i = 1}^{n}A_ih_i + \abs{\vect{h}}\rho(\vect{h})
\end{align*}
och $\limit{\vect{h}}{\vect{0}}\rho(\vect{h}) = 0$. $f$ är differentierbar om detta är uppfylld för alla $\vect{a}\in D$.

\paragraph{$C^1$}
Låt $f: D\to\R$ med $D\subset\R^n$. $f$ är klass $C^1$ om $f$ är partiellt deriverbar och alla de partiella derivatorna är kontinuerliga i $D$.

\paragraph{$C^k$}
Låt $f: D\to\R$ med $D\subset\R^n$. $f$ är klass $C^k$ om $f$ alla partiella derivator till och med ordning $k$ existerar och är kontinuerliga i $D$.

\paragraph{$C^1$-kurvor}
En kurva är klass $C^1$ om alla dess komponenter är $C^1$.

\paragraph{Gradient}
Låt $f$ vara reellvärd och differentierbar i $\vect{x}$. Gradienten definieras som
\begin{align*}
	\grad{f} = \sum\limits_{i = 1}^{n}\pdeval{f}{x_i}{\vect{x}}\vect{e}_i.
\end{align*}

\paragraph{Riktningsderivata}
Låt $\abs{\vect{v}} = 1$. Derivatan av $f$ i punkten $\vect{a}$ i riktningen $\vect{v}$ är
\begin{align*}
	\grad_{\vect{u}}f = \limit{t}{0}\frac{f(\vect{a} + t\vect{v}) - f(\vect{a})}{t}.
\end{align*}

\subsection{Satser}

\paragraph{Differentierbarhet och kontinuitet}
Låt $f$ vara differentierbar i $\vect{a}$. Då är $f$ kontinuerlig i $\vect{a}$.

\proof
Definitionen implicerar $\limit{\vect{h}}{\vect{0}}f(\vect{a} + \vect{h}) - f(\vect{a}) = 0$.

\paragraph{Differentierbarhet och partiell deriverbarhet}
Låt $f$ vara differentierbar i $\vect{a}$. Då är $f$ partiellt deriverbar med avseende på alla variabler i $\vect{a}$ och $\pdv{f}{x_i}(\vect{a}) = A_i$.

\proof
Med $\vect{h} = t\vect{e}_i$ ger definitionen av differentierbarhet
\begin{align*}
	\frac{f(\vect{a} + t\vect{e}_i) - f(\vect{a})}{t} = A_i + \frac{\abs{t}}{t}\rho(t\vect{e}_i).
\end{align*}
Gränsvärdet när $t$ går mot $0$ ger på den ena sidan definitionen av den partiella derivatan och $A_i$ på andra sidan.

\paragraph{Differentierbarhet av funktioner i $C^1$}
Varje $f\in C^1$ är differentierbar.

\proof
Låt $\vect{a}\in D$. Enligt envariabelsanalysens medelvärdesats har vi
\begin{align*}
	&f(\vect{a} + h_1\vect{e}_1) - f(\vect{a}) = \pdeval{f}{x_1}{\vect{a} + \theta _1h_1\vect{e}_1} \\
	&f(\vect{a} + h_1\vect{e}_1 + h_2\vect{e}_2) - f(\vect{a} + h_1\vect{e}_1) = \pdeval{f}{x_2}{\vect{a} + h_1\vect{e}_1 + \theta_2h_2\vect{e}_2} \\
	&\vdots \\
	&f(\vect{a} + \sum\limits_{i = 1}^{n}h_i\vect{e}_i) - f(\vect{a} + \sum\limits_{i = 1}^{n - 1}h_i\vect{e}_i) = \pdeval{f}{x_n}{\vect{a} + \sum\limits_{i = 1}^{n - 1}h_i\vect{e}_i + \theta_nh_n\vect{e}_n},
\end{align*}
där alla $\theta_i\in [0, 1]$. Eftersom de partiella derivatorna är kontinuerliga kan vi skriva
\begin{align*}
	\pdeval{f}{x_k}{\vect{a} + \sum\limits_{i = 1}^{k - 1}h_i\vect{e}_i + \theta_kh_k\vect{e}_k} = \pdeval{f}{x_k}{\vect{a}} + \rho_k(\sum\limits_{i = 1}^{n}h_i\vect{e}_i) = \pdeval{f}{x_k}{\vect{a}} + \rho_k(\vect{h}),
\end{align*}
där $\limit{\vect{h}}{\vect{0}}\rho(\vect{h}) = 0$. Då får man
\begin{align*}
	f(\vect{a} + \vect{h}) = \sum\limits_{i = 1}^{n}\left(\pdeval{f}{x_i}{\vect{a}} + \rho_i(\vect{h})\right)h_i.
\end{align*}

Den sista delen av beviset använder
\begin{align*}
	\limit{\vect{h}}{\vect{0}}\frac{\sum\limits_{i = 1}^{n}\rho_i(\vect{h})h_i}{\abs{\vect{h}}}.
\end{align*}

\paragraph{Allmänna kedjeregeln}
Låt $f: \R^n\to\R^p$ och $g: \R^q\to\R^n$ och låt alla komponenter av $f, g$ vara differentierbara. Då är alla komponenter av $f\circ g$ differentierbara. Med $u = f\circ g$ har vi
\begin{align*}
	\pdeval{u_i}{t_k}{\vect{t}} = \sum\limits_{i = 1}^{n}\pdeval{f}{x_i}{g(\vect{t})}\pdeval{g}{t_k}{\vect{t}}
\end{align*}
för varje komponent.

\paragraph{Specialfall: $p = 1$}
Låt $f$ vara en differentierbar funktion av $n$ variabler och $g: \R\to\R^n$, där alla $g_i$ är partiellt deriverbara. Då är $f\circ g$ deriverbar och
\begin{align*}
	\deval{f\circ g}{t}{t} = \sum\limits_{i = 1}^{n}\pdeval{f}{x_i}{g(t)}\deval{g_i}{t}{t}.
\end{align*}

\proof

\paragraph{Konstantfunktioner och gradient}
Låt $D\subset\R^n$ vara öppen och bågvis sammanhängande och $f\in C^1(D, R^n)$. Om $\grad{f(\vect{x})} = 0$ för alla $\vect{x}\in D$, är $f$ konstant i $D$.

\proof
Använd att
\begin{align*}
	\deval{f}{t}{\vect{x}(t)} = \grad{f(\vect{x}(t))}\cdot\deval{\vect{x}}{t}{t} = 0.
\end{align*}

\paragraph{Gradient och riktningsderivata}
Gradienten i riktning $\vect{v}$ ges av
\begin{align*}
	\grad_{\vect{v}}f(\vect{a}) = \grad{f}(\vect{a})\cdot\vect{v}.
\end{align*}

\proof
Bilda $u(t) = f(\vect{a} + t\vect{v}) = u(\vect{g}(t))$, vilket ger $\grad_{\vect{v}}f(\vect{a}) = \deval{u}{t}{0}$. Enligt kedjeregeln blir detta
\begin{align*}
	\sum\limits_{i = 1}^{n}\deval{f}{x_i}{0}\deval{g_i}{t}{0} = \grad{f}(\vect{a})\cdot\deval{(\vect{a} + t\vect{v})}{t}{0} = \grad{f}(\vect{a})\cdot\vect{v}.
\end{align*}

\paragraph{Maximal riktningsderivata}
$\grad{f(\vect{a})}$ pekar i den riktning i vilken $f$ växar snabbast i $\vect{a}$, och den maximala tillväxthastigheten är $\abs{\grad{f(\vect{a})}}$.

\proof
Cauchy-Schwarz-olikheten ger
\begin{align*}
	\grad_{\vect{u}}f = \grad{f}(\vect{a})\cdot\vect{v}\leq\abs{\grad{f}(\vect{a})}\abs{\vect{v}},
\end{align*}
med likhet om och endast om $\vect{v}$ är parallell med gradienten.

\paragraph{Gradient och nivåytor}
Låt $f: \R^n\to\R$ och $\grad{f}(\vect{a})\neq\vect{0}$. Då är gradienten normal på nivåytan $f(\vect{x}) = f(\vect{a})$.

\proof
Låt $\vect{x}(t)$ vara en $C^1$-kurva i nivåytan $f(\vect{x}) = f(\vect{a})$ så att $\vect{x}(0) = \vect{a}$. Detta ger
\begin{align*}
	0 = \deval{f\circ \vect{x}}{t}{0} = \grad{f}(\vect{a})\cdot\deval{\vect{x}}{t}{0}.
\end{align*}
Eftersom $\deval{\vect{x}}{t}{0}$ är parallell med nivåytan är beviset klart.

\paragraph{Symmetri av derivator i $C^2$}
För varje $f\in C^2$ gäller att
\begin{align*}
	\pdv{f}{x_i}{x_j} = \pdv{f}{x_j}{x_i}.
\end{align*}

\proof
Vi beviser endast för en tvåvariabelfunktion, då det allmänna fallet följer direkt från detta. Låt $q(h, k) = f(x + h, y + k) - f(x + h, y) - f(x, y + k) + f(x, y), \phi(t) = f(x + h, t) - f(x, t)$. Detta ger
\begin{align*}
	q(h, k) &= \phi(y + k) - \phi(y) \\
	        &= k\deval{\phi}{t}{y + \theta k} \\
	        &= k(\pdeval{f}{y}{x + h, y + \theta k} - \pdeval{f}{y}{x, y + \theta k}) \\
	        &= kh\pdv{f}{x}{y} (x + \eta h, y + \theta k),
\end{align*}
där vi har användt medelvärdesatsen två gånger. Då har vi
\begin{align*}
	\limit{(h, k)}{(0, 0)}\frac{q(h, k)}{hk} = \pdv{f}{x}{y} (x, y).
\end{align*}
Beviset kan upprepas i motsatt ordning, och detta fullförar beviset.