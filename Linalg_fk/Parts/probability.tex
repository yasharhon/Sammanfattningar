\section{Sannolikhet}

\subsection{Definitioner}

\paragraph{Sannolikhetsmatriser}
En sannolikhetsmatris är en kvadratisk matris där alla element är positiva och summan i varje kolonn är $1$.

\paragraph{Stokastiska processer}
En stokastisk process är en följd av stokastiska variabler.

\paragraph{Markovprocesser}
En Markovprocess är en stokastisk process som endast beror av förra steget i processen.

Om den stokastiska variabeln är på vektorform, kan rekursionen skrivas som
\begin{align*}
	X_{n + 1} = AX_{n}.
\end{align*}

\subsection{Satser}

\paragraph{Egenvärden till sannolikhetsmatriser}
$1$ är alltid ett egenvärde till en sannolikhetsmatris.

\proof
Låt $A$ vara en sannolikhetsmatris. Om $e = [1 \dots 1]^{T}$ är
\begin{align*}
	e^{T}A = e^{T}
\end{align*}
enligt definitionen, vilket implicerar
\begin{align*}
	A{T}e = e.
\end{align*}
Eftersom $\det{A - \lambda I} = \det{A^{T} - \lambda I}$ är beviset klart.

\paragraph{Perron-Frobenius' sats}
Om $A$ är en reguljär sannolikhetsmatris, dvs. uppfyller att $A^{m}$ för något $m$ bara har positiva element, gäller följande:
\begin{itemize}
	\item Det finns en egenvektor med egenvärde $1$ så att alla element i denna är positiva.
	\item Den algebraiska och geometriska multipliciteten till egenvärdet $1$ är båda lika med $1$.
	\item Om $\lambda$ är ett annat egenvärde är $\abs{\lambda} < 1$.
	\item Alla andra egenvektorer har koordinater som summerar till $1$.
\end{itemize}

\proof
Det finns ett $x\neq 0$ så att $Ax = x$. Vi bildar nu $x^{+}$ så att $x_{i}^{+} = \abs{x_{i}}$. Då kan vi skriva
\begin{align*}
	x     &= x_{+} - x_{-}, \\
	x^{+} &= x_{+} + x_{-}
\end{align*}
Där $x_{+}$ och $x_{-}$ innehåller de positiva respektiva negativa elementen i $x$. Detta ger
\begin{align*}
	Ax_{+} &= A(x + x_{-}) = x + Ax_{-} \geq x, \\
	Ax_{-} &= A(x_{+} - x) = Ax_{+} - x \geq -x, \\
	Ax^{+} &= A(x_{+} + x_{-}) = Ax_{+} + Ax_{-} \geq x^{+}
\end{align*}
där olikheterna jämför varje koordinat för sig. Vi inför igen vektorn $e^{T} = [1 \dots 1]$ och får
\begin{align*}
	e^{T}x^{+} \leq e^{T}Ax^{+} = e^{T}x^{+},
\end{align*}
alltså måste dessa vara lika och $Ax^{+} = x^{+}$, vilket beviser första påståendet.

Antag vidare att det finns en annan egenvektor $y$ motsvarande egenvärdet $1$. Det finns då ett $\alpha$ så att vektorn $x^{+} + \alpha y$ har en koordinat lika med $0$. Detta ger (säkert) motsägelse enligt argumentet ovan eftersom $(x^{+} + \alpha y)^{+} > 0$, vilket bevisar andra påståendet.

Vi definierar vidare $A_{\infty} = x^{+}e^{T}$ och får
\begin{align*}
	A_{\infty}^{2} = x^{+}e^{T}x^{+}e^{T}.
\end{align*}
Om $x^{+}$ är normerad så att summan av elementen är lika med $1$, ger detta
\begin{align*}
	A_{\infty}^{2} = x^{+}e^{T}x^{+}e^{T} = x^{+}e^{T} = A_{\infty}.
\end{align*}
Vi har vidare
\begin{align*}
	AA_{\infty} &= Ax^{+}e^{T} = x^{+}e^{T} = A_{\infty}, \\
	A_{\infty}A &= x^{+}e^{T}A = x^{+}e^{T} = A_{\infty}.
\end{align*}
Vi antar att alla element i $A$ är positiva, och därmed finns det ett $\varepsilon > 0$ så att $B = A - \varepsilon A_{\infty}$ också är en positiv matris. Vi har vidare
\begin{align*}
	B^{n} &= \sum\limits_{i = 0}^{n}{n\choose i}(-\varepsilon)^{i}A_{\infty}^{i}A^{n - i} \\
	      &= A^{n} + \sum\limits_{i = 1}^{n}{n\choose i}(-\varepsilon)^{i}A_{\infty}A^{n - i} \\
	      &= A^{n} + \sum\limits_{i = 1}^{n}{n\choose i}(-\varepsilon)^{i}A_{\infty} \\
	      &= A^{n} - A_{\infty} + \sum\limits_{i = 0}^{n}{n\choose i}(-\varepsilon)^{i} A_{\infty} \\
	      &= A^{n} - A_{\infty} + (1 - \varepsilon)^{n}A_{\infty}.
\end{align*}
Det gäller att $\lim\limits_{n\to\infty}B^{n} = 0$ eftersom kolumnerna i $B$ nu summerar till något mindre än $1$, vilket implicerar $\lim\limits_{n\to\infty}A^{n} = A_{\infty}$. Betrakta vidare en egenvektor motsvarande egenvärdet $\lambda$. Eftersom $\lim\limits_{n\to\infty}A^{n}$ existerar, existerar även $\lim\limits_{n\to\infty}A^{n}y = \lim\limits_{n\to\infty}\lambda^{n}y$. Detta är bara möjligt om $\abs{\lambda} < 1$ eller $\lambda = 1$ ($\abs{\lambda} = 1$ uppfylls ju bara om $\lambda = 1$, vilket redan har täckts).