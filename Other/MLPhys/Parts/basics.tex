\section{Basic Concepts}

\paragraph{Machine Learning}
Machine learning is the field of study that gives computers the ability to learn without explicit programming. A computer is said to learn from some experience $E$ with respekt to a task $T$ and a performance measure $P$ if its performance at $T$ as measured by $P$ increases with $E$.

\paragraph{Supervised Learning}
Supervised learning deals with labelled data, and the task is to correctly label unfamiliar data. The two main types of supervised learning problems are classification and regression.

\paragraph{Unsupervised Learning}
Unsupervised learning, bu contrast, deals with data that does not have labels. The task is generally to identify some patterns within the data. Examples of such problems include clustering, anomaly detection, pattern recognition and association mining.

\paragraph{Reinforcement Learning}
Reinforcement learning is, loosely speaking, based on performing random explorations of prediction processes and learning from the output.

\paragraph{A Note on Hardware}
Machine learning benefits strongly from parallelization. This is one of the reasons why, despite CPUs having high clock speeds, GPUs are the preferred hardware for training algorithms.

\paragraph{Using Machine Learning}
Machine learning is notoriously full of pitfalls. Great care must be taken in the preparatory work and the process of creating and training the model. The general steps in the procedure are
\begin{enumerate}
	\item \textbf{Define the problem and plan.} Formulate good questions and assess whether a machine learning approach is adequate and necessary.
	\item \textbf{Estimate the required computational resources.} Combined with economic aspects, this will determine the limits of the complexity of your model.
	\item \textbf{Prepare data.} This step is crucial and will include identifying sources, data collection and preprocessing. Relevant questions to ask include whether you have enough data (a quintessential point) and whether the data is sufficiently diverse.
	\item \textbf{Construct an appropriate model.}
	\item \textbf{Test the model.}
	\item \textbf{Deploy.}
\end{enumerate}

See also \href{https://karpathy.github.io/2019/04/25/recipe/}{this article} for a well-known and detailed example of how one should think.

\paragraph{Statistical Learning}
Statistical learning is a framework in which the tools of statistics are used to describe and evaluate a machine learning algorithm.

A useful starting point is the limit of large numbers, for which we can us the Markov density approximation
\begin{align*}
	P(X = x) \approx \frac{1}{N}\sum\limits_{l}\delta(x - x_{l}),
\end{align*}
which implies
\begin{align*}
	\expval{f(X)} \approx \frac{1}{N}\sum\limits_{l}f(x_{l}).
\end{align*}
This holds for identically distributed, independent realisation $x_{l}$ of $P(X = x)$.

Given this, the input to the learner is the domain set $X$, points in which are numerical representations of the data, the label space $Y$, which contains the possible labels that can be ascribed to a point in $X$, and a set $S$ of pairs $(x, y)\in X\times Y$, which is a realization of the joint probability distribution $P(X = x, Y = y) = f(x, y)$. The output is a prediction rule $h: X\to Y$.

We will need some way to assess the quality of $h$. This is done by introducing a loss function $L(h(x), y)$, which measures the ability of $h$ to predict $y$ given $x$. Next, we introduce the risk functional
\begin{align*}
	R(h) = \inte{}{}\dd{x}\dd{y}f(x, y)L(h(x), y) = \inte{}{}\dd{x}P(X = x)\inte{}{}\dd{y}P(Y = y\given X = x)L(h(x), y),
\end{align*}
which measures the expected loss. Assuming a good choice of the loss function, the best choice of $h$ is taken to be the one that minimizes the risk. Such a predictor is called Bayes optimal.

We take $f$ to be unknown, but assuming the training data to be sufficiently large we can construct a Markov density approximation for $f$. We then find
\begin{align*}
	R(h)\approx R_{S}(h) = \frac{1}{\abs{S}}\sum\limits_{i}\inte{}{}\dd{x}\dd{y}\delta(x - x_{i})\delta(y - y_{i})L(h(x), y) = \frac{1}{\abs{S}}\sum\limits_{i}L(h(x_{i}), y_{i}),
\end{align*}
which is the so-called empirical risk. We can now approximately identify an optimal choice of $h$ by minimizing this quantity. This process is called empirical risk minimization. The assumption that the class of hypotheses has a member that minimizes $R_{S}$ is the realizability assumption.

\paragraph{Bayes Optimal Predictors}
The Bayes optimal predictor is a predictor which, given the probability distribution $P(y = y\given X = x)$, labels according to
\begin{align*}
	f(x) = 
	\begin{cases}
		1,\ P(y = y\given X = x) \geq \frac{1}{2}, \\
		0,\ \text{otherwise}.
	\end{cases}
\end{align*}
This predictor turns out to minimize the true loss.

\paragraph{Probability Terms of Relevance}
A few terms of relevance are
\begin{itemize}
	\item accuracy, which is the fraction of predictions that is correct.
	\item precision, which is the fraction of positive predictions that corresponds to true positives.
	\item sensitivity, which is the fraction of true positive cases that is identified.
	\item specificity, which is the fraction of negative predictions that corresponds to true negatives.
\end{itemize}

\paragraph{Probably Approximately Correct Learning}
How wrong is the empirical approximation? It is clear that $R_{S}$ approaches $R$ in the limit of infinite sample sizes. For finite sample sizes, however, we can only make probabilistic statements. We introduce probably approximately correct (PAC) learning as follows: A hypothesis class $H$ is PAC learnable if there exists a number $m_{H}$ depending on two probabilities such that for every pair of probabilities $\epsilon, \delta$ and under the realizability assumption, when running the learning algorithm with $m\geq m_{H}(\epsilon, \delta)$ iid samples, the algorithm returns a hypothesis that satisfies
\begin{align*}
	P(\abs{R(h) - R_{s}(h)} > \epsilon) < \delta.
\end{align*}
It turns out that in the case of a finite hypothesis class, in the realizable case, we have
\begin{align*}
	m_{H}(\epsilon, \delta) = \frac{1}{\epsilon}\left(\ln(\abs{H}) + \ln(\frac{1}{\delta})\right).
\end{align*}

\paragraph{Agnostic PAC Learning}
%TODO: Add more
A hypothesis class is agnostically PAC learnable if there exists a number $m_{H}$ depending on two probabilities and a learning algorithm such that for every pair of probabilities $\epsilon, \delta$ and every distribution $P$ over $X\times Y$, when running the learning algorithm with $m\geq m_{H}(\epsilon, \delta)$ iid samples from $P$, it holds that
\begin{align*}
	P(L(h_{S}) - \min L(h) \leq \varepsilon) \geq 1 - \delta.
\end{align*}
In this case we instead have
\begin{align*}
	m_{H}(\epsilon, \delta) = \frac{1}{2\epsilon^{2}}\left(\ln(\abs{H}) + \ln(\frac{2}{\delta})\right).
\end{align*}

\paragraph{The Confusion Matrix}
The confusion matrix is a $2\times 2$ matrix for which the diagonal elements contain the true positives and negatives, the upper right contains the false positives and the lower left contains the false negatives. There are also extension for more advanced classification problems.

\paragraph{The No Free Lunch Theorem}
It has been shown that if no assumptions are made about the data, there is no reason to prefer any particular machine learning model. In other words, there is no universal learner (combination of model and learning algorithm) that performs equally well for all problems. This is the no free lunch theorem.

\paragraph{The Bias-Variance Tradeoff}
In practical contexts, it will be important to validate the performance of your model on a separate dataset. The error on the training set, or in-sample error, is called the bias and represents the best possible performance of the model. The out-of sample error, which is the error on the validation set, is more representative of the true performance. It is equal to the sum of the asymptotic error and the so-called variance. The two generally approach each other as the amount of data increases, but the out-of-sample error is generally greater than the in-sample one.

What happens if you increase model complexity? What will generally happen is that the bias will decrease and the variance will increase. The first happens because increasing complexity allows for capturing more nuances but leads to overfitting of the data. There is typically a sweet spot where the out-of-sample error is minimal.

\paragraph{Linear Predictors}
A linear predictor is an element of the hypothesis class with elements of the form
\begin{align*}
	h(x; w) = w^{T}x.
\end{align*}

\paragraph{Matrix Regularization}
Consider the problem
\begin{align*}
	Ax = b.
\end{align*}
In the cases where $A$ is not invertible, we have to do some modification of the problem. This is called regularization.