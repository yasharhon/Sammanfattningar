\section{Basic Concepts}

\paragraph{Machine Learning}
Machine learning is the field of study that gives computers the ability to learn without explicit programming. A computer is said to learn from some experience $E$ with respekt to a task $T$ and a performance measure $P$ if its performance at $T$ as measured by $P$ increases with $E$.

\paragraph{Supervised Learning}
Supervised learning deals with labelled data, and the task is to correctly label unfamiliar data. The two main types of supervised learning problems are classification and regression.

\paragraph{Unsupervised Learning}
Unsupervised learning, bu contrast, deals with data that does not have labels. The task is generally to identify some patterns within the data. Examples of such problems include clustering, anomaly detection, pattern recognition and association mining.

\paragraph{Reinforcement Learning}
Reinforcement learning is, loosely speaking, based on performing random explorations of prediction processes and learning from the output.

\paragraph{A Note on Hardware}
Machine learning benefits strongly from parallelization. This is one of the reasons why, despite CPUs having high clock speeds, GPUs are the preferred hardware for training algorithms.

\paragraph{Using Machine Learning}
Machine learning is notoriously full of pitfalls. Great care must be taken in the preparatory work and the process of creating and training the model. The general steps in the procedure are
\begin{enumerate}
	\item \textbf{Define the problem and plan.} Formulate good questions and assess whether a machine learning approach is adequate and necessary.
	\item \textbf{Estimate the required computational resources.} Combined with economic aspects, this will determine the limits of the complexity of your model.
	\item \textbf{Prepare data.} This step is crucial and will include identifying sources, data collection and preprocessing. Relevant questions to ask include whether you have enough data (a quintessential point) and whether the data is sufficiently diverse.
	\item \textbf{Construct an appropriate model.}
	\item \textbf{Test the model.}
	\item \textbf{Deploy.}
\end{enumerate}

See also \href{https://karpathy.github.io/2019/04/25/recipe/}{this article} for a well-known and detailed example of how one should think.

\paragraph{Statistical Learning}
Statistical learning is a framework in which the tools of statistics are used to describe and evaluate a machine learning algorithm.

A useful starting point is the limit of large numbers, for which we can us the Markov density approximation
\begin{align*}
	P(X = x) \approx \frac{1}{N}\sum\limits_{l}\delta(x - x_{l}),
\end{align*}
which implies
\begin{align*}
	\expval{f(X)} \approx \frac{1}{N}\sum\limits_{l}f(x_{l}).
\end{align*}
This holds for identically distributed, independent realisation $x_{l}$ of $P(X = x)$.

Given this, the input to the learner is the domain set $X$, points in which are numerical representations of the data, the label space $Y$, which contains the possible labels that can be ascribed to a point in $X$, and a set $S$ of pairs $(x, y)\in X\times Y$, which is a realization of the joint probability distribution $P(X = x, Y = y) = f(x, y)$. The output is a prediction rule $h: X\to Y$.

We will need some way to assess the quality of $h$. This is done by introducing a loss function $L(h(x), y)$, which measures the ability of $h$ to predict $y$ given $x$. Next, we introduce the risk functional
\begin{align*}
	R(h) = \inte{}{}\dd{x}\dd{y}f(x, y)L(h(x), y) = \inte{}{}\dd{x}P(X = x)\inte{}{}\dd{y}P(Y = y\given X = x)L(h(x), y),
\end{align*}
which measures the expected loss. Assuming a good choice of the loss function, the best choice of $h$ is taken to be the one that minimizes the risk. Such a predictor is called Bayes optimal.

We take $f$ to be unknown, but assuming the training data to be sufficiently large we can construct a Markov density approximation for $f$. We then find
\begin{align*}
	R(h)\approx R_{S}(h) = \frac{1}{\abs{S}}\sum\limits_{i}\inte{}{}\dd{x}\dd{y}\delta(x - x_{i})\delta(y - y_{i})L(h(x), y) = \frac{1}{\abs{S}}\sum\limits_{i}L(h(x_{i}), y_{i}),
\end{align*}
which is the so-called empirical risk. We can now approximately identify an optimal choice of $h$ by minimizing this quantity. This process is called empirical risk minimization. The assumption that the class of hypotheses has a member that minimizes $R_{S}$ is the realizability assumption.

\paragraph{Bayes Optimal Predictors}
The Bayes optimal predictor is a predictor which, given the probability distribution $P(y = y\given X = x)$, labels according to
\begin{align*}
	f(x) = 
	\begin{cases}
		1,\ P(y = y\given X = x) \geq \frac{1}{2}, \\
		0,\ \text{otherwise}.
	\end{cases}
\end{align*}
This predictor turns out to minimize the true loss.

\paragraph{Probability Terms of Relevance}
A few terms of relevance are
\begin{itemize}
	\item accuracy, which is the fraction of predictions that is correct.
	\item precision, which is the fraction of positive predictions that corresponds to true positives.
	\item sensitivity, which is the fraction of true positive cases that is identified.
	\item specificity, which is the fraction of negative predictions that corresponds to true negatives.
\end{itemize}

\paragraph{Probably Approximately Correct Learning}
How wrong is the empirical approximation? It is clear that $R_{S}$ approaches $R$ in the limit of infinite sample sizes. For finite sample sizes, however, we can only make probabilistic statements. We introduce probably approximately correct (PAC) learning as follows: A hypothesis class $H$ is PAC learnable if there exists a number $m_{H}$ depending on two probabilities such that for every pair of probabilities $\epsilon, \delta$ and under the realizability assumption, when running the learning algorithm with $m\geq m_{H}(\epsilon, \delta)$ iid samples, the algorithm returns a hypothesis that satisfies
\begin{align*}
	P(\abs{R(h) - R_{s}(h)} > \epsilon) < \delta.
\end{align*}
It turns out that in the case of a finite hypothesis class, in the realizable case, we have
\begin{align*}
	m_{H}(\epsilon, \delta) = \frac{1}{\epsilon}\left(\ln(\abs{H}) + \ln(\frac{1}{\delta})\right).
\end{align*}

\paragraph{Agnostic PAC Learning}
A hypothesis class is agnostically PAC learnable if there exists a number $m_{H}$ depending on two probabilities and a learning algorithm such that for every pair of probabilities $\epsilon, \delta$ and every distribution $P$ over $X\times Y$, when running the learning algorithm with $m\geq m_{H}(\epsilon, \delta)$ iid samples from $P$, it holds that
\begin{align*}
	P(L(h_{S}) - \min L(h) \leq \varepsilon) \geq 1 - \delta.
\end{align*}
In this case we instead have
\begin{align*}
	m_{H}(\epsilon, \delta) = \frac{1}{2\epsilon^{2}}\left(\ln(\abs{H}) + \ln(\frac{2}{\delta})\right).
\end{align*}

\paragraph{The Confusion Matrix}
The confusion matrix is a $2\times 2$ matrix for which the diagonal elements contain the true positives and negatives, the upper right contains the false positives and the lower left contains the false negatives. There are also extension for more advanced classification problems.

\paragraph{The No Free Lunch Theorem}
It has been shown that if no assumptions are made about the data, there is no reason to prefer any particular machine learning model. In other words, there is no universal learner (combination of model and learning algorithm) that performs equally well for all problems. This is the no free lunch theorem.

\paragraph{The Bias-Complexity Tradeoff}
The empirical risk can be divided into two terms. The first is the approximation error, which is equal to the minimal empirical risk within the chosen hypothesis class. The second is the estimation error, which is equal to the difference between the observed empirical risk and the approximation error. The former thus represents the limitations of the hypothesis class itself, while the latter represents the success of the algorithm in producing a hypothesis given the data.

How can you change these? The solution to reduce the approximation error is to change the model. At some point one has to make the model more complicated, however, and it can be hard for a learning algorithm to identify a good hypothesis given the data. This is a tradeoff between the bias in the choice of hypothesis class and the complexity of the model.

In practical contexts, it will be important to validate the performance of your model on a separate dataset. The bias-complexity tradeoff manifests here as a tradeoff between performance on the training and validation sets respectively. We here estimate the approximation error as the error on the training set and the estimation error as the error on the validation set. One typically optimizes with respect to the validation error to try to ensure generalizability.

\paragraph{Linear Predictors}
A linear predictor is an element of the hypothesis class with elements of the form
\begin{align*}
	h(x; w, b) = f(w^{T}x + b),
\end{align*}
which can of course be shortened to
\begin{align*}
	h(x; w) = f(w^{T}x)
\end{align*}
by extending $x$ and $w$. An often-seen choice is the logistic function
\begin{align*}
	\sigma(s) = \frac{1}{e + e^{-s}}.
\end{align*}

\paragraph{Matrix Regularization}
Consider the problem
\begin{align*}
	Ax = b.
\end{align*}
In the cases where $A$ is not invertible, we have to do some modification of the problem. This is called regularization.

\paragraph{Classification and Cross-Entropy}
Consider a classification problem with $N$ classes, and restrict your hypothesis class to some set of functions
\begin{align*}
	P(y^{n}\given x, w) = f_{w}^{n}(x)
\end{align*}
parametrized by the $w$. The likelihood of observing some given data set can be written
\begin{align*}
	L(\{y_{i}\}) = \prod\limits_{i}f_{w}^{n_{i}}(x_{i})\prod\limits_{n \neq n_{i}}(1 - f_{w}^{n}(x_{i})),\ y^{n_{i}} = 1,
\end{align*}
or more neatly,
\begin{align*}
	L(\{y_{i}\}) = \prod\limits_{i}\prod\limits_{n}(f_{w}^{n}(x_{i}))^{y_{i}^{n}}(1 - f_{w}^{n}(x_{i}))^{1 - y_{i}^{n}}.
\end{align*}
The log likelihood is
\begin{align*}
	\ln(L) = \sum\limits_{i}\sum\limits_{n}y_{i}^{n}\ln(f_{w}^{n}(x_{i})) + (1 - y_{i}^{n})\ln(1 - f_{w}^{n}(x_{i})),
\end{align*}
and its maximization is equivalent to minimizing the cross-entropy
\begin{align*}
	S = -\sum\limits_{i}\sum\limits_{n}y_{i}^{n}\ln(f_{w}^{n}(x_{i})) + (1 - y_{i}^{n})\ln(1 - f_{w}^{n}(x_{i})).
\end{align*}

\paragraph{Stochastic Gradient Descent}
Gradient descent is a suboptimal way of minimizing a function, so we will instead opt for a different method.

Suppose we divide our data into $\frac{M}{I}$ subsets indexed by a $k$. We denote each subset $B_{k}$. The empirical risk is then is then
\begin{align*}
	R_{S} = \sum\limits_{i\in B_{k}}l(x_{i}) + \sum\limits_{i\notin B_{k}}l(x_{i}).
\end{align*}
The former is the in-batch loss and the latter the error. Discarding the former we have
\begin{align*}
	\grad R_{S} \approx \sum\limits_{i\in B_{k}}\grad l(x_{i}).
\end{align*}
For iid batches this is an unbiased estimator of $\grad R_{S}$. The stochastic gradient descent algorithm thus involves doing one gradient descent per batch. Such a cycle is called an epoch. This is advantageous because it is faster and lessens the risk of getting stuck in local minima.