\section{Basic Concepts}

\paragraph{Machine Learning}
Machine learning is the field of study that gives computers the ability to learn without explicit programming. A computer is said to learn from some experience $E$ with respekt to a task $T$ and a performance measure $P$ if its performance at $T$ as measured by $P$ increases with $E$.

\paragraph{Supervised Learning}
Supervised learning deals with labelled data, and the task is to correctly label unfamiliar data. The two main types of supervised learning problems are classification and regression.

\paragraph{Unsupervised Learning}
Unsupervised learning, bu contrast, deals with data that does not have labels. The task is generally to identify some patterns within the data. Examples of such problems include clustering, anomaly detection, pattern recognition and association mining.

\paragraph{Reinforcement Learning}
Reinforcement learning is, loosely speaking, based on performing random explorations of prediction processes and learning from the output.

\paragraph{A Note on Hardware}
Machine learning benefits strongly from parallelization. This is one of the reasons why, despite CPUs having high clock speeds, GPUs are the preferred hardware for training algorithms.

\paragraph{Using Machine Learning}
Machine learning is notoriously full of pitfalls. Great care must be taken in the preparatory work and the process of creating and training the model. The general steps in the procedure are
\begin{enumerate}
	\item \textbf{Define the problem and plan.} Formulate good questions and assess whether a machine learning approach is adequate and necessary.
	\item \textbf{Estimate the required computational resources.} Combined with economic aspects, this will determine the limits of the complexity of your model.
	\item \textbf{Prepare data.} This step is crucial and will include identifying sources, data collection and preprocessing. Relevant questions to ask include whether you have enough data (a quintessential point) and whether the data is sufficiently diverse.
	\item \textbf{Construct an appropriate model.}
	\item \textbf{Test the model.}
	\item \textbf{Deploy.}
\end{enumerate}

See also \href{https://karpathy.github.io/2019/04/25/recipe/}{this article} for a well-known and detailed example of how one should think.

\paragraph{Statistical Learning}
Statistical learning is a framework in which the tools of statistics are used to describe and evaluate a machine learning algorithm.

A useful starting point is the limit of large numbers, for which we can us the Markov density approximation
\begin{align*}
	P(X = x) \approx \frac{1}{N}\sum\limits_{l}\delta(x - x_{l}),
\end{align*}
which implies
\begin{align*}
	\expval{f(X)} \approx \frac{1}{N}\sum\limits_{l}f(x_{l}).
\end{align*}
This holds for identically distributed, independent realisation $x_{l}$ of $P(X = x)$.

Given this, the input to the learner is the domain set $X$, points in which are numerical representations of the data, the label space $Y$, which contains the possible labels that can be ascribed to a point in $X$, and a set $S$ of pairs $(x, y)\in X\times Y$, which is a realization of the joint probability distribution $P(X = x, Y = y) = f(x, y)$. The output is a prediction rule $h: X\to Y$.

We will need some way to assess the quality of $h$. This is done by introducing a loss function $L(h(x), y)$, which measures the ability of $h$ to predict $y$ given $x$. Next, we introduce the risk functional
\begin{align*}
	R(h) = \inte{}{}\dd{x}\dd{y}f(x, y)L(h(x), y) = \inte{}{}\dd{x}P(X = x)\inte{}{}\dd{y}P(Y = y\given X = x)L(h(x), y),
\end{align*}
which measures the expected loss. Assuming a good choice of the loss function, the best choice of $h$ is taken to be the one that minimizes the risk. Such a predictor is called Bayes optimal.

We take $f$ to be unknown, but assuming the training data to be sufficiently large we can construct a Markov density approximation for $f$. We then find
\begin{align*}
	R(h)\approx R_{S}(h) = \frac{1}{\abs{S}}\sum\limits_{i}\inte{}{}\dd{x}\dd{y}\delta(x - x_{i})\delta(y - y_{i})L(h(x), y) = \frac{1}{\abs{S}}\sum\limits_{i}L(h(x_{i}), y_{i}),
\end{align*}
which is the so-called empirical risk. We can now approximately identify an optimal choice of $h$ by minimizing this quantity. This process is called empirical risk minimization. The assumption that the class of hypotheses has a member that minimizes $R_{S}$ is the realizability assumption.

\paragraph{Bayes Optimal Predictors}
The Bayes optimal predictor is a predictor which, given the probability distribution $P(y = y\given X = x)$, labels according to
\begin{align*}
	f(x) = 
	\begin{cases}
		1,\ P(y = y\given X = x) \geq \frac{1}{2}, \\
		0,\ \text{otherwise}.
	\end{cases}
\end{align*}
This predictor turns out to minimize the true loss.

\paragraph{Probability Terms of Relevance}
A few terms of relevance are
\begin{itemize}
	\item accuracy, which is the fraction of predictions that is correct.
	\item precision, which is the fraction of positive predictions that corresponds to true positives.
	\item sensitivity, which is the fraction of true positive cases that is identified.
	\item specificity, which is the fraction of negative predictions that corresponds to true negatives.
\end{itemize}

\paragraph{Probably Approximately Correct Learning}
How wrong is the empirical approximation? It is clear that $R_{S}$ approaches $R$ in the limit of infinite sample sizes. For finite sample sizes, however, we can only make probabilistic statements. We introduce probably approximately correct (PAC) learning as follows: A hypothesis class $H$ is PAC learnable if there exists a number $m_{H}$ depending on two probabilities such that for every pair of probabilities $\epsilon, \delta$ and under the realizability assumption, when running the learning algorithm with $m\geq m_{H}(\epsilon, \delta)$ iid samples, the algorithm returns a hypothesis that satisfies
\begin{align*}
	P(\abs{R(h) - R_{s}(h)} > \epsilon) < \delta.
\end{align*}
It turns out that in the case of a finite hypothesis class, in the realizable case, we have
\begin{align*}
	m_{H}(\epsilon, \delta) = \frac{1}{\epsilon}\left(\ln(\abs{H}) + \ln(\frac{1}{\delta})\right).
\end{align*}

\paragraph{Agnostic PAC Learning}
A hypothesis class is agnostically PAC learnable if there exists a number $m_{H}$ depending on two probabilities and a learning algorithm such that for every pair of probabilities $\epsilon, \delta$ and every distribution $P$ over $X\times Y$, when running the learning algorithm with $m\geq m_{H}(\epsilon, \delta)$ iid samples from $P$, it holds that
\begin{align*}
	P(L(h_{S}) - \min L(h) \leq \varepsilon) \geq 1 - \delta.
\end{align*}
In this case we instead have
\begin{align*}
	m_{H}(\epsilon, \delta) = \frac{1}{2\epsilon^{2}}\left(\ln(\abs{H}) + \ln(\frac{2}{\delta})\right).
\end{align*}

\paragraph{The Confusion Matrix}
The confusion matrix is a $2\times 2$ matrix for which the diagonal elements contain the true positives and negatives, the upper right contains the false positives and the lower left contains the false negatives. There are also extension for more advanced classification problems.

\paragraph{The No Free Lunch Theorem}
It has been shown that if no assumptions are made about the data, there is no reason to prefer any particular machine learning model. In other words, there is no universal learner (combination of model and learning algorithm) that performs equally well for all problems. This is the no free lunch theorem.

\paragraph{The Bias-Complexity Tradeoff}
The empirical risk can be divided into two terms. The first is the approximation error, which is equal to the minimal empirical risk within the chosen hypothesis class. The second is the estimation error, which is equal to the difference between the observed empirical risk and the approximation error. The former thus represents the limitations of the hypothesis class itself, while the latter represents the success of the algorithm in producing a hypothesis given the data.

How can you change these? The solution to reduce the approximation error is to change the model. At some point one has to make the model more complicated, however, and it can be hard for a learning algorithm to identify a good hypothesis given the data. This is a tradeoff between the bias in the choice of hypothesis class and the complexity of the model.

In practical contexts, it will be important to validate the performance of your model on a separate dataset. The bias-complexity tradeoff manifests here as a tradeoff between performance on the training and validation sets respectively. We here estimate the approximation error as the error on the training set and the estimation error as the error on the validation set. One typically optimizes with respect to the validation error to try to ensure generalizability.

\paragraph{Linear Predictors}
A linear predictor is an element of the hypothesis class with elements of the form
\begin{align*}
	h(x; w, b) = f(w^{T}x + b),
\end{align*}
which can of course be shortened to
\begin{align*}
	h(x; w) = f(w^{T}x)
\end{align*}
by extending $x$ and $w$. An often-seen choice is the logistic function
\begin{align*}
	\sigma(s) = \frac{1}{e + e^{-s}}.
\end{align*}

\paragraph{Matrix Regularization}
Consider the problem
\begin{align*}
	Ax = b.
\end{align*}
In the cases where $A$ is not invertible, we have to do some modification of the problem. This is called regularization.

\paragraph{Classification and Cross-Entropy}
Consider a classification problem with $N$ classes, and restrict your hypothesis class to some set of functions
\begin{align*}
	P(y^{n}\given x, w) = f_{w}^{n}(x)
\end{align*}
parametrized by the $w$. The likelihood of observing some given data set can be written
\begin{align*}
	L(\{y_{i}\}) = \prod\limits_{i}f_{w}^{n_{i}}(x_{i})\prod\limits_{n \neq n_{i}}(1 - f_{w}^{n}(x_{i})),\ y^{n_{i}} = 1,
\end{align*}
or more neatly,
\begin{align*}
	L(\{y_{i}\}) = \prod\limits_{i}\prod\limits_{n}(f_{w}^{n}(x_{i}))^{y_{i}^{n}}(1 - f_{w}^{n}(x_{i}))^{1 - y_{i}^{n}}.
\end{align*}
The log likelihood is
\begin{align*}
	\ln(L) = \sum\limits_{i}\sum\limits_{n}y_{i}^{n}\ln(f_{w}^{n}(x_{i})) + (1 - y_{i}^{n})\ln(1 - f_{w}^{n}(x_{i})),
\end{align*}
and its maximization is equivalent to minimizing the cross-entropy
\begin{align*}
	S = -\sum\limits_{i}\sum\limits_{n}y_{i}^{n}\ln(f_{w}^{n}(x_{i})) + (1 - y_{i}^{n})\ln(1 - f_{w}^{n}(x_{i})).
\end{align*}

\paragraph{Stochastic Gradient Descent}
Gradient descent is a suboptimal way of minimizing a function, so we will instead opt for a different method.

Suppose we divide our data into $\frac{M}{I}$ subsets indexed by a $k$. We denote each subset $B_{k}$. The empirical risk is then is then
\begin{align*}
	R_{S} = \sum\limits_{i\in B_{k}}l(x_{i}) + \sum\limits_{i\notin B_{k}}l(x_{i}).
\end{align*}
The former is the in-batch loss and the latter the error. Discarding the former we have
\begin{align*}
	\grad R_{S} \approx \sum\limits_{i\in B_{k}}\grad l(x_{i}).
\end{align*}
For iid batches this is an unbiased estimator of $\grad R_{S}$. The stochastic gradient descent algorithm thus involves doing one gradient descent per batch. Such a cycle is called an epoch. This is advantageous because it is faster and lessens the risk of getting stuck in local minima.

\paragraph{Neural Networks}
Neural networks are elements in a very particular hypothesis class. They are conceptually inspired by neurons.

The structure is as follows: The network consists of layers. Each layer consists of a set of neurons. The neurons are generally coupled to all neurons in the previous layers. Computationally, each neuron takes as input the set of values stored in each neuron. It then adds them using some weights and outputs the result when this weighted sum is input into some so-called activation function. We may thus illustrate the network as a directed graph, where each neuron is a node and there are edges connecting all nodes in adjacent layers. These edges are directed, such that there is only coupling one way.

As an example, consider the simplest possible neural network, with one input layer and one output layer, the latter having only a single neuron. The input layer will then consist of all the elements in $x$ (as well as a $1$), and the neuron will first compute $\sum w_{i}x_{i} = w^{T}x$, and output $\sigma(w^{T}x)$.

The power of neural networks lies in the fact that if the activation function is nonlinear, then the network can be used to perform nonlinear classification or regression tasks. The design of the network, however, is where the art lies.

Neural networks need to be trained, of course. The algorithm for doing that is called back-propagation. To describe the mathematical basis we introduce a bit of notation. We let:
\begin{itemize}
	\item $V_{t}$ be the set of neurons in layer $t$.
	\item $W_{t}$ be the weight matrix between $V_{t}$ and $V_{t + 1}$.
	\item $k_{t}$ be the number of neurons in layer $t$.
	\item $\vb{a}_{t}$ be the input to layer $t$.
	\item $\vb{o}_{t}$ be the output from layer $t$.
	\item $\vb*{\sigma}$ be the activation function applied element-wise to the output of one layer.
\end{itemize}
Clearly we have $\vb{a}_{t} = W_{t - 1}\vb{o}_{t - 1}$ and $\vb{o}_{t} = \vb*{\sigma}(\vb{a}_{t})$. The weights in any one layer appear exactly once in the output, so it would be beneficial to switch from a matrix notation in $W$ to a vector notation. To that end, introduce the matrix $O_{t - 1} = \text{diag}(\vb{o}_{t - 1}^{T})$ and $\vb{w}_{t - 1}$ the vector obtained when concatenating the rows of $W_{t - 1}$. It then follows that $O_{t - 1}\vb{w}_{t - 1} = W_{t - 1}\vb{o}_{t - 1}$. Now, we note that the loss function has a recursive structure - that is,
\begin{align*}
	L(\{W_{t}\}) =& L(\vb{o}_{T}) \\
	             =& L(\vb*{\sigma}(\vb{a}_{T})) \\
	             =& L(\vb*{\sigma}(W_{T - 1}\vb{o}_{T - 1})) \\
	             =& \dots
\end{align*}
We can write this as $L = \ell_{t}\circ \vb*{\sigma}$ for some choice of $t$. $\ell_{t}$ thus acts as a subsystem loss function, handling the contributions to the loss from layers starting at $t$. We then have
\begin{align*}
	\grad_{\vb{w}_{t - 1}}L = \grad_{\vb*{\sigma}}\ell_{t}\pdv{\vb*{\sigma}}{\vb{w}_{t - 1}}.
\end{align*}
Because the activation function acts element-wise we have
\begin{align*}
	\pdv{\vb*{\sigma}}{\vb{w}_{t - 1}} = \text{diag}(\grad\sigma)\pdv{\vb{w}_{t - 1}}(O_{t - 1}\vb{w}_{t - 1}) = \text{diag}(\grad\sigma)O_{t - 1}.
\end{align*}
The first factor is to be evaluated at $\vb*{\sigma}(O_{t - 1}\vb{w}_{t - 1}) = \vb{o}_{t}$ and the second at $O_{t - 1}\vb{w}_{t - 1} = \vb{a}_{t}$. Thus we find
\begin{align*}
	\grad_{\vb{w}_{t - 1}}L = \grad_{\vb*{\sigma}}\ell_{t}(\vb{o}_{t})\text{diag}(\grad\sigma(\vb{a}_{t}))O_{t - 1}.
\end{align*}
This is a vector comprised of the successive components of $\vb*{\delta}$, the derivatives of $\sigma$ and $\vb{o}_{t - 1}$. Thus, by computing $\vb{\delta}_{t}$ the derivative is known. We can do this in a recursive manner by noting that it is trivial for the outer layer. Next, as
\begin{align*}
	\ell_{t}(\vb{o}_{t}) = \ell_{t + 1}(\vb*{\sigma}(W_{t}\vb{o}_{t})),
\end{align*}
hence
\begin{align*}
	\grad\ell_{t}(\vb{o}_{t}) = \grad\ell_{t + 1}(\vb{o}_{t + 1})\text{diag}(\grad\sigma(\vb{a}_{t + 1}))W_{t},
\end{align*}
implying
\begin{align*}
	\vb*{\delta}_{t} = \vb*{\delta}_{t + 1}\text{diag}(\grad\sigma(\vb{a}_{t + 1}))W_{t}.
\end{align*}
In this sense we propagate information back through the network. The algorithm is thus
\begin{enumerate}
	\item Start with $\vb{o}_{0} = \vb{x}$.
	\item Compute $\vb{a}_{t} = W_{t - 1}\vb{o}_{t - 1}$ and $\vb{o}_{t} = \vb*{\sigma}(\vb{a}_{t})$ for each layer.
	\item Set $\vb*{\delta}_{T} = \grad{L}(\vb{o}_{T})$.
	\item Compute $\vb*{\delta}_{t} = \vb*{\delta}_{t + 1}\text{diag}(\grad\sigma(\vb{a}_{t + 1}))W_{t}$ for each layer.
	\item Set the partial derivative with respect to each set of weights to be $\vb*{\delta}_{t}\text{diag}(\grad\sigma(\vb{a}_{t}))O_{t - 1}$.
\end{enumerate}

\paragraph{Problems With Training Neural Networks}
A first problem with training neural networks is vanishing or exploding gradients. To illustrate it, consider a neural network with one neuron per layer. We then have
\begin{align*}
	\vb*{\delta}_{0}  = \vb*{\delta}_{L}\prod\limits_{l = 1}^{L}\text{diag}(\grad\sigma(\vb{a}_{l}))W_{l}.
\end{align*}
For a network with many layers the product can thus very quickly either diverge or drop to zero. Only if each factor is close to the identity will the gradients stay behaved. Beyond applying stochastic gradient descent, which is an implicit regularizer, there are other treatments of the gradient that can be performed to ensure that this holds.

Another problem is that different weights can change such that they compensate for mistakes in other places in the neural networks. This is called co-adaption, and it is bad because such fixes are generally overfitted to the training data. This can be solved by using dropout.

Next there is the problem that layer inputs change during training, requiring low learning rates. This can be somewhat circumvented by standardizing inputs.

\paragraph{The Universal Approximation Theorem}
The universal approximation theorem is a statement about how closely we can approximate functions with neural networks. To prove it, consider a so-called feed-forward neural network with $L$ neurons in a single hidden layer. Its output is
\begin{align*}
	y(\vb{x}) = \sum\limits_{l = 0}^{L}\alpha_{l}\sigma(\vb{w}_{l}^{T}\vb{x} + b_{l}),
\end{align*}
where we have used some sigmoid activation function (this just means that $\sigma$ goes to $0$ and $1$ respectively at the two infinities). We also introduce the following definition: $\sigma$ is discriminatory for a measure $\mu\in M(l_{d})$ (this can for instance be a probability measure $\dd{\mu} = \dd{\vb{x}}P(\vb{x})$) if the fact that
\begin{align*}
	\inte{}{}\dd{\mu}\sigma(\vb{w}_{l}^{T}\vb{x} + b_{l}) = 0
\end{align*}
for all $\vb{w}_{l}, b_{l}$ implies that $\mu = 0$. This definition is useful because it tells us that discriminatory functions are non-destructive with respect to the metric - that is, they are indeed able to capture some of the information in the network.

The theorem states the following: Let $l_{d}$ be the hypercube $[0, 1]^{d}$ and the space of real-valued continuous functions on $l_{d}$ be $C(l_{d})$. Let also $\sigma$ be a discriminatory function on $l_{d}$. Then, for every $\varepsilon > 0$ and function $f\in C(l_{d})$, there exists an integer $N$ and real $\alpha_{l}, b_{l}, \vb{w}_{l}$ for $l = 1, \dots, N$ such that the function
\begin{align*}
	G(\vb{x}) = \sum\limits_{l = 0}^{L}\alpha_{l}\sigma(\vb{w}_{l}^{T}\vb{x} + b_{l})
\end{align*}
satisfies
\begin{align*}
	\abs{G(\vb{x}) - f(\vb{x})} < \varepsilon\ \forall\ \vb{x}\in l_{d}.
\end{align*}

%TODO: Add proof
To prove it, let $S$ be the set of functions of the form $G$. Clearly this is a linear subspace of $C(l_{d})$. To prove it, we will show that the closure of $S$ (the smallest closed set containing $S$) is all of $C(l_{d})$. Assuming the contrary, the closure of $S$ is instead some different subset $R$ of $C(l_{d})$. We have by the Hahn-Banach theorem that there is a bounded non-trivial linear functional $L$ of $C(l_{d})$ such that $L(R) = L(S) = 0$. This can be represented as
\begin{align*}
	L(h) = \inte{l_{d}}{}\dd{\mu}h(\vb{x})
\end{align*}
for some measure $\mu$ and all $h$. By our assumptions there must then exist a measure such that
\begin{align*}
	L(\sigma) = \inte{l_{d}}{}\dd{\mu}\sigma(w_{l}^{T}\vb{x} + b_{l}) = 0,
\end{align*}
but if $\sigma$ is discriminatory, then this is a contradiction. This implies that $R$ must indeed be $C(l_{d})$, completing the proof.