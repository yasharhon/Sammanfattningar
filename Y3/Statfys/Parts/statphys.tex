\section{Statistical Physics}

\paragraph{Probability and Observables}
The purpose of statistical mechanics is to treat systems in a statistical manner. We will describe systems in terms of probability distributions in phase space and compute observables as expectation values from this probability. A first step to note is that, in terms of classical mechanics, these observables must be computed as time averages. However, the fundamental postulate allows us to replace this by ensemble averages. We thus obtain the formula
\begin{align*}
	\expval{A} = \sum A_{i}P_{i}
\end{align*}
where the summation is performed over all of phase space.

\subsection{The Microcanonical Ensemble}

\paragraph{The Multiplicity}
There are two ways of working with the microcanonical ensemble: One is to use the probability distribution $\rho = c\delta(H - E)$, where $H$ is the Hamiltonian. The second, which allows us to more easily extend the arguments to quantum systems, is to allow a tolerance $\delta E$ in energy upwards from $E$ and use a uniform probability distribution in this region. In either case, we now introduce the multiplicity of a given macrostate as
\begin{align*}
	\Omega = \frac{1}{h^{dn}N!}\integ[dN]{E < H < E + \delta E}{}{x}{},
\end{align*}
where $x$ is a phase space position variable. The approach that we have used allows $\Omega$ to be a continuous function of the total energy even for quantum systems, which have discrete, closely-spaced energy levels.

The factor $h$ is some factor with dimension $pq$ in classical context, but will be revealed to be Planck's constant. The factor $N!$ accounts for the fact that the particles are indistinguishable.

\paragraph{Distinguishability}
For indistinguishable particles, an the need for the factor $N!$ is due to overcounting of states where two or more particles have different energies. For such states, switching the places of two such particles produces the same state. The overcounting is by a factor of $n!$, where $n$ is the number of particles with different energies. However, if states where two or more particles have the same energy can be ignored, we can solve the overcounting problem by accounting for the degeneracy of the remaining states by divding by $N!$.

But hold on! In classical mechanics, the equations of motions themselves imply that all particles are distinguishable. After all, any set of positions and momenta are directly connected to a single particle. How can the issue of distinguishability be relevant? The answer is that in the beginning, this factor was added ad hoc in order to guarantee that the entropy would be extensive. At its heart, however, the idea of indistinguishable particles is quantum mechanical. This could be taken as an early sign of the strong connection between statistical physics and quantum mechanics.

\paragraph{The Statistical Basis For Entropy}
To connect the microscopic picture to thermodynamics, we need Boltzmann's famous definition of entropy, given by
\begin{align*}
	S = \kb\ln{\Omega}.
\end{align*}
An ad hoc way of arriving at this result is to consider the entropy as a function of the multiplicity. Supposing that we have a system comprised of two non-correlated parts, the extensivity of the entropy yields
\begin{align*}
	S = S_{1} + S_{2}.
\end{align*}
Now, the total multiplicity is $\Omega = \Omega_{1}\Omega_{2}$, so requiring the entropy to be extensive takes the form
\begin{align*}
	S(\Omega_{1}\Omega_{2}) = S(\Omega_{1}) + S(\Omega_{2}).
\end{align*}
A simple choice of entropy function is thus to choose one among the elementary functions. The function which does this is the natural logarithm. The Boltzmann constant is now introduced as the constant of proportionality required to give $S$ a dimension.

\paragraph{Entropy and Probability}
Consider a system with $N$ available microstates, all equally likely and indistinguishable by experiment. These are divided into groups called to macrostates, distinguishable by experiment, with $n_{i}$ microstates in each macrostate. We have
\begin{align*}
	\sum n_{i} = N,
\end{align*}
and the probability of finding the system in a particular macrostate is thus
\begin{align*}
	P_{i} = \frac{n_{i}}{N}.
\end{align*}
The total entropy is $S_{\text{tot}} = \kb\ln{N}$, but cannot be measured as the microstates are indistinguishable. Nevertheless, as entropy is extensive, the total entropy is a sum of the freedom to explore the microstates for a given macrostate and the freedom to explore the different macrostates, i.e.
\begin{align*}
	S_{\text{tot}} = S + S_{\text{micro}}.
\end{align*}
The entropy due to exploring microstates is an expectation value over the different macrostates of the entropy, given by
\begin{align*}
	S_{\text{micro}} = \sum P_{i}S_{i} = \kb\sum P_{i}\ln{n_{i}}.
\end{align*}
This yields
\begin{align*}
	S &= S_{\text{tot}} - S_{\text{micro}} \\
      &= \kb\ln{N} - \kb\sum P_{i}\ln{n_{i}} \\
      &= \kb\sum P_{i}\left(\ln{N} - \ln{n_{i}}\right) \\
      &= -\kb\sum P_{i}\ln{P_{i}}.
\end{align*}
This expression might be useful in some contexts.

\paragraph{The Ideal Gas}

\subsection{The Canonical Ensemble}

\paragraph{Temperature Balance From Microscopic Arguments}
Consider two isolated systems in contact. The total energy and multiplicity is given by
\begin{align*}
	E = E_{1} + E_{2},\ \Omega \propto \integ{E < E_{\text{T}} < E + \delta E}{}{E_{\text{T}}}{\integ{-\infty}}{E_{\text{T}}}{E_{1}}{\Omega_{2}(E_{\text{T}} - E_{1})\Omega_{1}(E_{1})}.
\end{align*}
This function is often sharply peaked around some particular $E_{1}$, as both multiplicities are rapidly increasing functions. Hence the total entropy is
\begin{align*}
	S_{1 + 2}(E, E_{1}) = S_{1}(E_{1}) + S_{2}(E - E_{1}).
\end{align*}
The peak value corresponds to a maximal entropy. Differentiating with respect to $E_{1}$ gives
\begin{align*}
	\pdv{S_{1 + 2}}{E_{1}}                 &= \dv{S_{1}}{E_{1}} + \dv{S_{2}}{E_{2}}\dv{E_{2}}{E_{1}} = \dv{S_{1}}{E_{1}} - \dv{S_{2}}{E_{2}},
\end{align*}
and using the thermodynamic definition of temperature we obtain
\begin{align*}
	\frac{1}{T_{1}} = \frac{1}{T_{2}}.
\end{align*}

\paragraph{The Boltzmann Distribution}
Consider again the previously described situation such that system $2$ is much larger than system $1$. We have
\begin{align*}
	p(E_{1})\dd{E_{1}} = \frac{\Omega_{2}(E - E_{1})\Omega_{1}(E_{1})}{\integ{}{}{E_{1}}{\Omega_{2}(E - E_{1})\Omega_{1}(E_{1})}}\dd{E_{1}}.
\end{align*}
The entropy is extensive, hence we have
\begin{align*}
	\Omega_{2}(E - E_{1}) = e^{\frac{S_{2}(E - E_{1})}{\kb}}.
\end{align*}
The entropy may be expanded in a series as
\begin{align*}
	S_{2}(E - E_{1}) &= S_{2}(E) - \pdv{S_{2}}{E}E_{1} + \frac{1}{2}\pdv[2]{S_{2}}{E}E_{1}^{2} + \dots \\
	                 &= S_{2}(E) - \frac{1}{T}E_{1} - \frac{1}{2}\frac{1}{T^{2}}\fix{\pdv{E}\left(T\right)}{V_{2}, N_{2}}E_{1}^{2} + \dots \\
	                 &= S_{2}(E) - \frac{1}{T}E_{1} - \frac{1}{2}\frac{1}{T^{2}C_{2}}E_{1}^{2} + \dots
\end{align*}
As system $2$ is very large, the higher-order terms may be neglected, yielding
\begin{align*}
	\Omega_{2}(E - E_{1}) = Ce^{-\frac{E_{1}}{\kb T}}.
\end{align*}
Finally we obtain
\begin{align*}
	p(E_{1})\dd{E_{1}} = \frac{Ce^{-\frac{E_{1}}{\kb T}}\Omega_{1}(E_{1})}{\integ{}{}{E_{1}}{Ce^{-\frac{E_{1}}{\kb T}}\Omega_{1}(E_{1})}}\dd{E_{1}}.
\end{align*}
This is the Boltzmann distribution.

\paragraph{The Partition Function}
The partition function is defined as
\begin{align*}
	Z = \integ{}{}{E_{1}}{Ce^{-\beta E_{1}}\Omega_{1}(E_{1})},
\end{align*}
where the $E_{i}$ are possible values of the energy (or, rather, the Hamiltonian) and we now formally introduce $\beta = \frac{1}{\kb T}$. An equivalent definition is
\begin{align*}
	Z = \sum e^{-\beta E_{i}},
\end{align*}
where the summation is performed over all of phase space.

\paragraph{Properties of the Partition Function}
Suppose that the Hamiltonian is redefined by adding a constant. We then obtain
\begin{align*}
	Z' = \sum e^{-\beta (E_{i} + E_{0})} = e^{-\beta E_{0}}Z.
\end{align*}
The value of any observable is thus
\begin{align*}
	\expval{A'} = \sum A'_{i}P'_{i} = \sum A'_{i}\frac{e^{-\beta(E_{i} + E_{0})}}{e^{-\beta E_{0}}Z} = \frac{1}{Z}\sum A'_{i}e^{-\beta E_{i}}.
\end{align*}
We see that the only way for the expectation value to change is if $A_{i}$ itself depends on the energy (as is the case if $A$ is the internal energy, for instance). Otherwise, we have
\begin{align*}
	\expval{A'} = \expval{A}.
\end{align*}

Suppose that the Hamiltonian is separable, i.e. can be written as a sum over Hamiltonians $H_{j}$ describing the $j$th degree of freedom. We thus obtain
\begin{align*}
	Z = \sum e^{-\sum\beta H_{j}} = \sum\limits_{\text{dof 1}}\dots\sum\limits_{\text{dof }N}e^{-\sum\beta H_{j}} = \prod\limits_{j}\sum\limits_{\text{dof j}}e^{-\beta H_{j}} = \prod\limits_{j}Z_{j}
\end{align*}
where each partition function describes a given degree of freedom. This also has the consequence that observables which only depend on a subset of the degrees of freedom may be computed without considering the other degrees of freedom.

\paragraph{Distinguishability}
For distinguishable particles (distinguished by, for instance, spatial separation) of the same kind we have
\begin{align*}
	Z_{N} = Z_{1}^{N}.
\end{align*}
For indistinguishable particles we instead have
\begin{align*}
	Z_{N} = \frac{Z_{1}^{N}}{N!}.
\end{align*}

\paragraph{Helmholtz Free Energy}
The probability distribution may be written as
\begin{align*}
	p(E) \propto e^{-\beta(E - TS(E))}.
\end{align*}
A large system is overwhelmingly likely found in the state that maximizes the probability, namely the state such that the exponent is maximal. Expanding around this minimum yields
\begin{align*}
	-\beta(E - TS(E)) \approx -\beta(\expval{E} - TS(\expval{E})) + \frac{1}{2\kb}(E - \expval{E})^{2}\pdv[2]{S}{E} + \dots
\end{align*}
The partition function may now be written as
\begin{align*}
	Z = e^{-\beta(\expval{E} - TS(\expval{E}))}\integ{}{}{E}{Ce^{\frac{1}{2\kb}(E - \expval{E})^{2}\pdv[2]{S}{E}(E - \expval{E})^{2} + \dots}} = \sqrt{2\pi\kb T^{2}C}e^{-\beta(\expval{E} - TS(\expval{E}))}.
\end{align*}
Its logarithm contains one term which grows much slower than the others, and may thus be ignored. Hence we have
\begin{align*}
	-\kb T\ln{Z} = \expval{E} - TS(\expval{E}) = F.
\end{align*}

\paragraph{Internal Energy}
In a statistical context, we define the internal energy as $U = \expval{E}$. We have
\begin{align*}
	U &= \frac{\sum E_{i}e^{-\beta E_{i}}}{\sum e^{-\beta E_{i}}} \\
	  &= \frac{-\dv{Z}{\beta}}{Z} \\
	  &= -\pdv{\ln{Z}}{\beta}.
\end{align*}
Alternatively, in terms of temperature,
\begin{align*}
	U = -\pdv{\ln{Z}}{T}\pdv{T}{\beta} = \frac{1}{\kb\beta^{2}}\pdv{\ln{Z}}{T} = \kb T^{2}\pdv{\ln{Z}}{T}.
\end{align*}

\paragraph{Heat Capacity}
The heat capacity is given by
\begin{align*}
	C = \pdv{U}{T} = 2\kb T\pdv{\ln{Z}}{T} + \kb T^{2}\pdv[2]{\ln{Z}}{T}.
\end{align*}
Alternatively, we may write
\begin{align*}
	C &= \pdv{T}\left(\sum E_{i}\frac{1}{Z}e^{-\beta E_{i}}\right) \\
	  &= \frac{Z\cdot\frac{1}{\kb T^{2}}\sum E_{i}^{2}e^{-\beta E_{i}} - \pdv{Z}{T}\sum E_{i}e^{-\beta E_{i}}}{Z^{2}} \\
	  &= \frac{1}{\kb T^{2}}\expval{E^{2}} + \frac{\expval{E}}{Z}\cdot -\frac{1}{\kb T^{2}}\pdv{Z}{\beta} \\
	  &= \frac{1}{\kb T^{2}}\left(\expval{E^{2}} - \expval{E}^{2}\right).
\end{align*}
In other words, heat capacities correspond to energy fluctuations.

\paragraph{Heat Capacities With Discrete Energy Levels}
For systems with discrete energies the energy fluctuations must vanish at low temperatures, as only the lowest energy states becomes accessible.

\paragraph{Pressure}
The pressure is given by
\begin{align*}
	p = -\pdv{F}{V} = \frac{1}{\beta}\pdv{\ln{Z}}{V}.
\end{align*}

\paragraph{Enthalpy}
The enthalpy is given by
\begin{align*}
	H = \kb T^{2}\pdv{\ln{Z}}{T} + \frac{V}{\beta}\pdv{\ln{Z}}{V} = \kb T\left(T\pdv{\ln{Z}}{T} + V\pdv{\ln{Z}}{V}\right).
\end{align*}

\paragraph{Gibbs Free Energy}
The Gibbs free energy is given by
\begin{align*}
	G = \kb T\left(-\ln{Z} + V\pdv{\ln{Z}}{V}\right).
\end{align*}

\paragraph{The Equipartition Theorem}
Suppose that some degree of freedom $x$, which may take any value, of a system without degenerate microstates only appears in the Hamiltonian as a term $\alpha\abs{x}^{\nu}$. Its contribution to the internal energy is
\begin{align*}
	\expval{\alpha\abs{x}^{\nu}} &= \frac{\integ{-\infty}{\infty}{x}{\alpha\abs{x}^{\nu}e^{-\beta\alpha\abs{x}^{\nu}}}}{\integ{-\infty}{\infty}{x}{e^{-\beta\alpha\abs{x}^{n}}}} \\
	                             &= \frac{\integ{0}{\infty}{x}{\alpha x^{n}e^{-\beta\alpha x^{n}}}}{\integ{0}{\infty}{x}{e^{-\beta\alpha x^{n}}}}.
\end{align*}
Defining
\begin{align*}
	I_{\nu} = \integ{0}{\infty}{x}{e^{-\beta\alpha x^{n}}}
\end{align*}
we can write
\begin{align*}
	\expval{\alpha\abs{x}^{\nu}} = -\frac{1}{I_{\nu}}\dv{I_{\nu}}{\beta} = -\dv{\ln{I_{\nu}}}{\beta}.
\end{align*}
We have
\begin{align*}
	I_{\nu} &= \integ{0}{\infty}{u}{\frac{1}{\nu\beta\alpha}x^{1 - \nu}e^{-u}} \\
	        &= \frac{1}{\nu\beta\alpha}\integ{0}{\infty}{u}{\left(\frac{u}{\beta\alpha}\right)^{\frac{1}{\nu} - 1}e^{-u}} \\
	        &= \frac{1}{\nu}\left(\frac{1}{\beta\alpha}\right)^{\frac{1}{\nu}}\integ{0}{\infty}{u}{u^{\frac{1}{\nu} - 1}e^{-u}} \\
	        &= \beta^{-\frac{1}{\nu}}\frac{\Gamma\left(\frac{1}{\nu}\right)}{\nu\alpha^{\frac{1}{\nu}}}.
\end{align*}
From this we obtain
\begin{align*}
	\expval{\alpha\abs{x}^{\nu}} = \frac{1}{\nu\beta} = \frac{1}{\nu}\kb T.
\end{align*}

Examples of such degrees of freedom (assuming a sufficient number of states is thermally available, which may not be the case for systems with a quantum nature) are
\begin{itemize}
	\item translations, rotations and vibrations in a gas.
	\item vibrations in a crystal.
\end{itemize}

\paragraph{The Ideal Gas}
We will now study the statistical mechanics of an ideal gas. We will do this by considering the gas as a set of non-interacting free quantum particles in an $L\times L\times L$ box. The single-state wave function is
\begin{align*}
	\psi = \left(\frac{2}{L}\right)\sin(k_{x}x)\sin(k_{y}y)\sin(k_{z}z),
\end{align*}
where each quantum number is given by $k_{i} = \frac{\pi}{L}n_{i}$. Representing each possible state in a $3$-dimensional space where each point represents a sate, the available states of the system occupy the first octant, each state taking up a cube with dimensions $\frac{\pi}{L}$. The number of states with wave vector of length $k$ to $k + \dd{k}$ is given by $\frac{1}{8}4\pi k^{2}\dd{k}$. Defining the density of states $g$ as the number of states in a small $k$-interval divided by the interval length and the volume occupied by each state implies 
\begin{align*}
	g(k) = \frac{\frac{1}{2}\pi k^{2}}{\left(\frac{\pi}{L}\right)^{3}} = \frac{Vk^{2}}{2\pi^{2}}.
\end{align*}
The single-particle partition function can now be calculated as
\begin{align*}
	Z_{1} = \integ{0}{\infty}{k}{e^{-\beta E}g(k)} = \integ{0}{\infty}{k}{e^{-\beta\frac{\hbar^{2}k^{2}}{2m}}\frac{Vk^{2}}{2\pi^{2}}} = \frac{V}{2\pi^{2}}\integ{0}{\infty}{k}{e^{-\beta\frac{\hbar^{2}k^{2}}{2m}}k^{2}} = \frac{V}{\hbar^{3}}\left(\frac{m\kb T}{2\pi}\right)^{\frac{3}{2}}.
\end{align*}
We can now define the quantum concentration
\begin{align*}
	n_{\text{Q}} = \left(\frac{m\kb T}{2\pi\hbar^{2}}\right)^{\frac{3}{2}}
\end{align*}
and the thermal wavelength
\begin{align*}
	\lambda_{\text{th}} = \frac{1}{n_{\text{Q}}^{\frac{1}{3}}},
\end{align*}
allowing us to write
\begin{align*}
	Z_{1} = Vn_{\text{Q}} = \frac{V}{\lambda_{\text{th}}^{3}}.
\end{align*}
The partition function for the entire system can be written as
\begin{align*}
	Z_{N} = \frac{1}{N!}\frac{V^{N}}{\lambda_{\text{th}}^{3N}}
\end{align*}
assuming that the number of thermally accessible energy levels is much larger than the density of particles, i.e. $\frac{N}{V} << n_{\text{Q}}$.

It is now time to derive thermodynamic properties of the system. We have
\begin{align*}
	U = \frac{3}{2}\kb T,\ p = \frac{N\kb T}{V},\ S = N\kb\left(\frac{5}{2} - \ln(\frac{N}{V}\lambda_{\text{th}}^{3})\right),\ G = N\kb T\ln(\frac{N}{V}\lambda_{\text{th}}^{3}).
\end{align*}

\paragraph{Heat Capacity of a Diatomic Gas}
A major challenge of classical thermodynamics was explaining the behaviour of the heat capacity of diatomic gases. A diatomic ideal gas has translational, vibrational and rotational degrees of freedom. Classical equipartition theory would imply their heat capacity to be $C_{V} = \frac{7}{2}\kb T$, but this was only observed after heating above a certain temperature.

To understand this we have to study diatomic gases with quantum mechanics. We find that higher rotational energy levels are only thermally accessible for temperatures close to or above $\frac{\hbar^{2}}{2I\kb}$, and vibrational levels close to or above $\frac{\hbar\omega}{\kb}$.

\subsection{The Grand Canonical Ensemble}

\paragraph{The Grand Canonical Probability Distribution}
The derivation performed to obtain the canonical partition function was not unique to that transformation. The microcanonical ensemble is specified only in terms of extensive variables, but a switch could be made to any number of intensive variables. This will be demonstrated for the grand canonical ensemble, where the number of particles in a system is allowed to vary.

To show this, consider two systems in contact such that they may exchange energy and particles only between themselves. The total multiplicity is given by
\begin{align*}
	\Omega \propto \sum\limits_{N_{1} = 0}^{N}\integ{E < E_{\text{T}} < E + \delta E}{}{E_{\text{T}}}{\integ{-\infty}}{E_{\text{T}}}{E_{1}}{\Omega_{2}(E_{\text{T}} - E_{1}, N - N_{1})\Omega_{1}(E_{1}, N_{1})}.
\end{align*}
This function is often sharply peaked around some particular $E_{1}$ and $N_{1}$. Hence the total entropy is
\begin{align*}
	S_{1 + 2}(E_{\text{T}}, E_{1}, N, N_{1}) = S_{1}(E_{1}, N_{1}) + S_{2}(E_{\text{T}} - E_{1}, N - N_{1}).
\end{align*}
The peak value corresponds to a maximal entropy. Differentiating with respect to $E_{1}$ gives
\begin{align*}
	\pdv{S_{1 + 2}}{E_{1}} = \pdv{S_{1}}{E_{1}} + \pdv{S_{2}}{E_{2}}\dv{E_{2}}{E_{1}} = \pdv{S_{1}}{E_{1}} - \pdv{S_{2}}{E_{2}} = 0,\ \frac{1}{T_{1}} = \frac{1}{T_{2}}.
\end{align*}
Similarly, with respect to the number of particles we have
\begin{align*}
	\pdv{S_{1 + 2}}{N_{1}} = \pdv{S_{1}}{N_{1}} + \pdv{S_{2}}{N_{2}}\dv{N_{2}}{N_{1}} = \pdv{S_{1}}{N_{1}} - \pdv{S_{2}}{N_{2}} = 0,\ \pdv{S_{1}}{N_{1}} = \pdv{S_{2}}{N_{2}}.
\end{align*}
The first and second laws of thermodynamics yield
\begin{align*}
	\dd{U} = T\dd{S} + \mu\dd{N}.
\end{align*}
The derivatives with respect to numbers are taken such that energy is fixed. Hence we have
\begin{align*}
	T\fix{\pdv{S}{N}}{U} + \mu = 0,\ \fix{\pdv{S}{N}}{U} = -\frac{\mu}{T}
\end{align*}
and the equilibrium condition
\begin{align*}
	\frac{\mu_{1}}{T_{1}} = \frac{\mu_{2}}{T_{2}}.
\end{align*}

Suppose now that system $2$ is much larger than system $1$. We have
\begin{align*}
	p(E_{1}, N_{1})\dd{E_{1}} = \frac{\Omega_{2}(E - E_{1}, N - N_{1})\Omega_{1}(E_{1}, N_{1})}{\sum\limits_{N_{1} = 0}^{N}\integ{}{}{E_{1}}{\Omega_{2}(E - E_{1}, N - N_{1})\Omega_{1}(E_{1}, N_{1})}}\dd{E_{1}}.
\end{align*}
The entropy is extensive, hence we have
\begin{align*}
	\Omega_{2}(E - E_{1}) = e^{\frac{S_{2}(E - E_{1}, N - N_{1})}{\kb}}.
\end{align*}
This may be expanded in a series as
\begin{align*}
	S_{2}(E - E_{1}) &= S_{2}(E, N) - \pdv{S_{2}}{E}E_{1} + \frac{1}{2}\pdv[2]{S_{2}}{E}E_{1}^{2} - \pdv{S_{2}}{N}N_{1} + \frac{1}{2}\pdv[2]{S_{2}}{N}N_{1}^{2} + \dots \\
	                  &= S_{2}(E, N) - \frac{1}{T}E_{1} - \frac{1}{2}\frac{1}{T^{2}}\fix{\pdv{T}{E}}{N_{2}}E_{1}^{2} + \frac{\mu}{T}N_{1} + \frac{1}{2}\fix{\pdv{E}\left(-\frac{\mu}{T}\right)}{T_{2}}N_{1}^{2} + \dots \\
	                  &= S_{2}(E, N) - \frac{1}{T}(E_{1} - \mu N_{1}) - \frac{1}{2T}\left(\frac{1}{C_{2}T}E_{1}^{2} + \fix{\pdv{\mu}{E}}{T_{2}}N_{1}^{2}\right) + \dots
\end{align*}
As system $2$ is very large, the higher-order terms may be neglected, yielding
\begin{align*}
	\Omega_{2}(E - E_{1}, N - N_{1}) = e^{\frac{1}{\kb}\left(S_{2}(E, N) - \frac{1}{T}(E_{1} - \mu N_{1})\right)} = Ce^{-\frac{1}{\kb T}(E_{1} - \mu N_{1})}.
\end{align*}
Finally we obtain
\begin{align*}
	p(E_{1}, N_{1})\dd{E_{1}} = \frac{e^{-\frac{1}{\kb T}(E_{1} - \mu N_{1})}\Omega_{1}(E_{1}, N_{1})}{\sum\limits_{N_{1} = 0}^{N}\integ{}{}{E_{1}}{e^{-\frac{1}{\kb T}(E_{1} - \mu N_{1})}\Omega_{1}(E_{1}, N_{1})}}\dd{E_{1}}.
\end{align*}
This is the grand canonical probability distribution, sometimes termed the Gibbs distribution.

\paragraph{The Grand Canonical Partition Function}
The grand canonical partition function is defined as
\begin{align*}
	\Z = \sum\limits_{N = 0}^{\infty}\integ{}{}{E}{e^{-\beta(E - \mu N)}\Omega_{1}(E, N)}.
\end{align*}
The number of particles in the system and reservoir is typically very large and large $N$ add small terms to the sum, hence it may be extended to infinity. This may also be written as
\begin{align*}
	\Z = \sum\limits_{N = 0}^{\infty}e^{\beta\mu N}\integ{}{}{E}{e^{-\beta E}\Omega_{1}(E, N)} = \sum\limits_{N = 0}^{\infty}e^{\beta\mu N}Z_{N}.
\end{align*}

\paragraph{The Grand Potential}
The probability distribution may be written as
\begin{align*}
	p(E, N) \propto e^{-\beta(E - TS - \mu N)}.
\end{align*}
A large system is overwhelmingly likely to be found in the state that maximizes the exponent. Expanding around this maximum yields
\begin{align*}
	-\beta(E - TS - \mu N) \approx& -\beta(\expval{E} - T\expval{S} - \mu \expval{N}) \\
	                              &+ \frac{1}{2}\left(\fix{\pdv[2]{E}\left(-\beta(E - TS - \mu N)\right)}{N}(E - \expval{E})^{2} +  \fix{\pdv[2]{N}\left(-\beta(E - TS - \mu N)\right)}{E}(N - \expval{N})^{2}\right) \\
	                              &+ \pdv{}{E}{N}\left(-\beta(E - TS - \mu N)\right)(E - \expval{E})(N - \expval{N}).
\end{align*}
Letting $\vb{w}$ be a vector containing the fluctuations and $H_{\GP}$ the Hessian matrix of the grand potential with the given restrictions, the higher-order terms take the form of a quadratic form. The grand canonical partition function is thus
\begin{align*}
	\Z &= \sum\limits_{N = 0}^{\infty}\integ{}{}{E}{e^{-\beta(\expval{\GP}) + \frac{1}{2}\vb{w}^{T}H_{\GP}\vb{w}}} \\
	   &= e^{-\beta\expval{\GP}}\sum\limits_{N = 0}^{\infty}\integ{}{}{E}{e^{-\frac{1}{2}\beta\vb{w}^{T}H_{\GP}\vb{w}}}.
\end{align*}
The total exponent must be negative definite, hence $\Z$ converges. To compute it, the sum may be approximated as an integral. A change of variables yields
\begin{align*}
	\Z \propto e^{-\beta\expval{\GP}}\sqrt{\frac{1}{\beta^{2}\lambda_{1}\lambda_{2}}},
\end{align*}
where $\lambda_{i}$ are the eigenvalues of $H_{\GP}$. These are equal to the determinant of $H_{\GP}$, hence we have
\begin{align*}
	\ln(\Z) = C - \beta\expval{\GP} + \ln(\frac{\kb}{E_{0}\sqrt{\fix{\pdv[2]{S}{E}}{N}\fix{\pdv[2]{S}{N}}{E} - \left(\pdv{S}{E}{N}\right)^{2}}}).
\end{align*}
Entropy is extensive, hence the argument of the logarithm can at most be proportional to $N$. In the thermodynamic limit we thus have
\begin{align*}
	\GP = -\kb T\ln(Z).
\end{align*}

An important point is that this procedure, apart from the summation over $N$ instead of integration, applies to any case where you want to switch from controlling an extensive variable to the conjugate intensive variable. The only special case is the one where $T$ is controlled instead of $U$, but that case has been handled explicitly.

%TODO: Fluctuations in extensive variable

\paragraph{Fugacity}
We now introduce the often-occuring fugacity
\begin{align*}
	z = e^{\beta\mu}.
\end{align*}

\paragraph{Number of Particles}
The number of particles is given by
\begin{align*}
	N = \frac{1}{\Z}\sum N_{i}e^{-\beta(E_{i} - \mu N_{i})} = \frac{1}{\Z}\pdv{\Z}{\beta\mu} = \kb T\pdv{\ln{\Z}}{\mu}.
\end{align*}

\paragraph{Internal Energy}
The internal energy is given by
\begin{align*}
	U = \frac{1}{\Z}\sum E_{i}e^{-\beta(E_{i} - \mu N_{i})} = \frac{1}{\Z}\left(-\pdv{\Z}{\beta} + \sum\mu N_{i}e^{-\beta(E_{i} - \mu N_{i})}\right) = -\pdv{\ln{\Z}}{\beta} + \mu N.
\end{align*}

\paragraph{Entropy}
The entropy is given by
\begin{align*}
	S &= -\kb\sum P_{i}\ln{P_{i}} \\
	  &= \kb\sum \frac{1}{\Z}e^{-\beta(E_{i} - \mu N_{i})}\cdot\left(\beta(E_{i} - \mu N_{i}) - \ln{\Z}\right) \\
	  &= \frac{U}{T} - \frac{\mu N}{T} - \kb\sum P_{i}\ln{\Z} \\
	  &= \frac{U - \mu N + \kb T\ln{\Z}}{T}.
\end{align*}

\paragraph{Grand Potential}
The grand potential is given by
\begin{align*}
	\GP = U - TS - \mu N = -\kb T\ln{\Z}.
\end{align*}

%TODO: Fugacity