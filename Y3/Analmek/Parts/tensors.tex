\section{Tensors}

\paragraph{Definition}
A tensor of rank $N$ is a multilinear map from $N$ vectors to a scalar.

\paragraph{Components of a Tensor}
The components of a tensor are defined by
\begin{align*}
	T(\vb{E}^{a_{1}}, \dots, \vb{E}^{a_{N}}) = T^{a_{1}, \dots, a_{N}}.
\end{align*}
These are called the contravariant components of the tensor, and the covariant components are defined similarly. Mixed components can also be defined.

\paragraph{Basic Operations on Tensors}
Tensors obey the following rules:
\begin{align*}
	(T_{1} + T_{2})(\vb{w}_{1}, \dots, \vb{w}_{n_{2}}) &= T_{1}(\vb{w}_{1}, \dots, \vb{w}_{n_{2}}) + T_{2}(\vb{w}_{1}, \dots, \vb{w}_{n_{2}}), \\
	(kT)(\vb{w}_{1}, \dots, \vb{w}_{n_{2}})            &= kT(\vb{w}_{1}, \dots, \vb{w}_{n_{2}}).
\end{align*}
In component form:
\begin{align*}
	(T_{1} + T_{2})^{a_{1}\dots a_{n}} &= T_{1}^{a_{1}\dots a_{n}} + T_{2}^{a_{1}\dots a_{n}}, \\
	(kT)^{a_{1}\dots a_{n}}            &= kT^{a_{1}\dots a_{n}}.
\end{align*}

\paragraph{Coordinate Transformations}
Under a change of coordinates, the components of the tensor then transform according to
\begin{align*}
	T^{a_{1}\dots a_{n}} &= T((\tb{a_{1}})', \dots, (\tb{a_{n}})') \\
	                     &= T(L_{a_{1}}^{b_{1}}\tb{b_{1}}, \dots, L_{a_{n}}^{b_{n}}\tb{b_{n}}) \\
	                     &= L_{a_{1}}^{b_{1}}\dots L_{a_{n}}^{b_{n}}T(\tb{b_{1}}, \dots, \tb{b_{n}}) \\
	                     &= L_{a_{1}}^{b_{1}}\dots L_{a_{n}}^{b_{n}}T^{b_{1}, \dots, b_{n}}.
\end{align*}
Many introductions to tensors define tensors according to this relation. And now you know where it comes from.

\paragraph{The Tensor Product}
Given two tensors $T_{1}$ and $T_{2}$ of ranks $n_{1}$ and $n_{2}$, we kan define the rank $n_{1} + n_{2}$ tensor $\tenprod{{T_{1}, T_{2}}}$ as
\begin{align*}
	(\tenprod{{T_{1}, T_{2}}})(\vb{v}_{1}, \dots, \vb{v}_{n_{1}}, \vb{w}_{1}, \dots, \vb{w}_{n_{2}}) = T_{1}(\vb{v}_{1}, \dots, \vb{v}_{n_{1}})T_{2}(\vb{w}_{1}, \dots, \vb{w}_{n_{2}}).
\end{align*}
In component form:
\begin{align*}
	(\tenprod{{T_{1}, T_{2}}})^{a_{1}\dots a_{n_{1} + n_{2}}} = T_{1}^{a_{1}\dots a_{n_{1}}}T_{2}^{a_{n_{1} + 1}\dots a_{n_{1} + n_{2}}}.
\end{align*}

\paragraph{A Base For The Space of Tensors}
Using the tensor product, all tensors can be written as linear combinations of certain basis elements due to their bilinearity. Define
\begin{align*}
	e_{a_{1}\dots a_{n}} = \tenprod{{\vb{E}_{a_{1}}, \dots, \vb{E}_{a_{n}}}}
\end{align*}
to be the tensor that satisfies
\begin{align*}
	e_{a_{1}\dots a_{n}}(\vb{E}^{b_{1}}, \dots, \vb{E}^{b_{n}}) = (\vb{E}_{a_{1}}\cdot\vb{E}^{b_{1}})\dots (\vb{E}_{a_{n}}\cdot\vb{E}^{b_{n}}) = \kdelta{a_{1}}{b_{1}}\dots\kdelta{a_{n}}{b_{n}}.
\end{align*}
Then any tensor can be written as
\begin{align*}
	T = T^{a_{1}\dots a_{n}}e_{a_{1}\dots a_{n}}
\end{align*}
where the $T^{a_{1}\dots a_{n}}$ are exactly the contravariant components of $T$.

\paragraph{Tensors as Linear Transforms Between Tensors}
A rank $n$ tensor can also be viewed as a linear map from rank $m$ tensors to rank $n - m$ tensors. To do this, we first define, given $T$, the rank $n - m$ tensor $\tilde{T}(\tenprod{{\vb{w}_{1}, \dots, \vb{w}_{m}}})$ such that
\begin{align*}
	(\tilde{T}(\tenprod{{\vb{w}_{1}, \dots, \vb{w}_{m}}}))(\vb{v}_{1}, \dots, \vb{v}_{n - m}) = T(\vb{w}_{1}, \dots, \vb{w}_{m}, \vb{v}_{1}, \dots, \vb{v}_{n - m}).
\end{align*}
This map is also linear in all the $\vb{w}_{i}$. Next, given a rank $n - m$ tensor $\tilde{T}$, one can define the rank $n - m$ tensor $T(\vb{w}_{1}, \dots, \vb{w}_{m})$ such that
\begin{align*}
	T(\vb{w}_{1}, \dots, \vb{w}_{m}, \vb{v}_{1}, \dots, \vb{v}_{n - m}) = (\tilde{T}(\tenprod{{\vb{w}_{1}, \dots, \vb{w}_{m}}}))(\vb{v}_{1}, \dots, \vb{v}_{n - m}).
\end{align*}
This is a linear rank $n$ tensor.

\paragraph{More General Tensors}
To expand on the above, we first define tensors as we have discussed them of $(0, n)$ tensors. We now phrase the above as follows: Given a $(0, n +m)$ tensor $T$ and a $(0, m)$ tensor $\tenprod{{\vb{w}_{1}, \dots, \vb{w}_{m}}}$ we may construct the tensor
\begin{align*}
	(\tilde{T}(\tenprod{{\vb{w}_{1}, \dots, \vb{w}_{m}}}))(\vb{1}, \dots, \vb{n}) = T(\vb{w}_{1}, \dots, \vb{w}_{m}, \vb{1}, \dots, \vb{n}),
\end{align*}
which is $(0, m)$. These must needs be linear.

Explicitly in terms of the contravariant tensor components we may construct such tensors as
\begin{align*}
	T = \tensor*{T}{^{a_{1}\dots a_{n}}_{b_{1}\dots b_{m}}}\bigotimes\limits_{k = 1}^{n}\tb{a_{k}}\bigotimes\limits_{l = 1}^{m}\db{b_{l}}.
\end{align*}
The components of such a tensor are
\begin{align*}
	T(e^{a_{1}\dots a_{n}})\left(\tb{b_{1}}, \dots, \tb{b_{m}}\right) = \tensor*{T}{^{a_{1}\dots a_{n}}_{b_{1}\dots b_{m}}}e^{a_{1}\dots a_{n}}\left(\bigotimes\limits_{k = 1}^{n}\tb{c_{k}}\right)\bigotimes\limits_{l = 1}^{m}\tb{b_{l}}\left(\db{d_{1}}, \dots, \db{d_{m}}\right) = \tensor*{T}{^{a_{1}\dots a_{n}}_{b_{1}\dots b_{m}}}.
\end{align*}

Recalling the transformation rules
\begin{align*}
	(\tb{b})' = L_{b}^{a}\tb{a},\ L_{b}^{a} = \del[\prime]{b}{\chi^{a}}
\end{align*}
and
\begin{align*}
	(\db{b})' = K_{a}^{b}\db{a},\ K_{a}^{b} = \del{a}{(\chi')^{b}}
\end{align*}
we may obtain general transformation rules for a $(n, m)$ tensor. We first have
\begin{align*}
	(e^{\prime})^{a_{1}\dots a_{n}} = \bigotimes\limits_{k = 1}^{n}(\db{a_{k}})^{\prime} = \bigotimes\limits_{k = 1}^{n}\del{c_{k}}{(\chi')^{a_{k}}}\db{c_{k}} = \left(\prod\limits_{k = 1}^{n}\del{c_{k}}{(\chi')^{a_{k}}}\right)\bigotimes\limits_{k = 1}^{n}\db{c_{k}} = \left(\prod\limits_{k = 1}^{n}\del{c_{k}}{(\chi')^{a_{k}}}\right)e^{c_{1}\dots c_{n}}.
\end{align*}
This yields
\begin{align*}
	\tensor*{(T^{\prime})}{^{a_{1}\dots a_{n}}_{b_{1}\dots b_{m}}} &= T((e^{\prime})^{a_{1}\dots a_{n}})\left((\tb{b_{1}})^{\prime}, \dots, (\tb{b_{m}})^{\prime}\right)\\
	&= T\left(\left(\prod\limits_{k = 1}^{n}\del{c_{k}}{(\chi')^{a_{k}}}\right)e^{c_{1}\dots c_{n}}\right)\left(\del[\prime]{b_{1}}{\chi^{d_{1}}}\tb{d_{1}}, \dots, \del[\prime]{b_{m}}{\chi^{d_{m}}}\tb{d_{m}}\right) \\
	&= \left(\prod\limits_{k = 1}^{n}\del{c_{k}}{(\chi')^{a_{k}}}\right)\left(\prod\limits_{l = 1}^{m}\del[\prime]{b_{l}}{\chi^{d_{l}}}\right)T\left(e^{c_{1}\dots c_{n}}\right)\left(\tb{d_{1}}, \dots, \tb{d_{m}}\right) \\
	&= \left(\prod\limits_{k = 1}^{n}\del{c_{k}}{(\chi')^{a_{k}}}\right)\left(\prod\limits_{l = 1}^{m}\del[\prime]{b_{l}}{\chi^{d_{l}}}\right)\tensor*{T}{^{c_{1}\dots c_{n}}_{d_{1}\dots d_{m}}}.
\end{align*}

\paragraph{Tensor Contraction}
Given a complete set of vectors $\vb{v}_{i}$ and their dual $\vb{v}^{i}$ such that $\vb{v}_{i}\cdot\vb{v}^{i} = \kdelta{i}{j}$, the contraction $e_{12}T$ of two arguments of a rank $n$ tensor is the tensor of rank $n - 2$ satisfying
\begin{align*}
	(e_{12}T)(\vb{w}_{1}, \dots, \vb{w}_{n - 2}) = T(\vb{v}_{i}, \vb{v}^{i}, \vb{w}_{1}, \dots, \vb{w}_{n - 2}).
\end{align*}
In component form:
\begin{align*}
	(e_{12}T)^{a_{1}\dots a_{n - 2}} = T^{c\;a_{1}\dots a_{n - 2}}_{\;c}.
\end{align*}
The definition is similar (I assume) for the contraction of other arguments.

\paragraph{Tensor Fields}
A tensor field is a map from coordinate space to a tensor.