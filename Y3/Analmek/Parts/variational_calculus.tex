\section{Variational Calculus}

\paragraph{Functionals}
A functional is a mapping from a function space to (real) numbers. The most general functional is composed of two components: One which takes a function in some function space and maps it to a function in another (these two spaces might, but need not, coincide), and one that takes a function in the second space and maps it to a number.

\paragraph{Functional Derivatives}
We will be concerned with optimizing functionals, hence we need some notion of derivatives of functionals with respect to their arguments. To express this in a way that is manageable with our knowledge of mathematics, we will transform the functional to a function of one variable. We do this by considering paths in function space parametrized by some parameter $\alpha$ and define the functional derivative as a directional derivative along that path. The paths of interest will be those where the shift is by some Dirac delta function. More explicitly, for a functional $S(\phi)$ we define
\begin{align*}
	\eval{\fdv{S}{\phi}}_{\phi = \phi_{\alpha}} = \lim\limits_{\epsilon\to 0}\frac{S(\phi_{\alpha + \epsilon}) - S(\phi_{\alpha})}{\epsilon}.
\end{align*}
For a functional depending on multiple fields, we compute the functional derivative along a path where only one function is varied. A similar thing may be done for a function map.

Because we have reduced the problem to a single-variable calculus one, theorems such as the chain rule still apply. We therefore only really need to calculate the functional derivative of a few basic function maps. Let us thus consider the function map
\begin{align*}
	\lag(\phi^{a})(x) = (\del{}{\mu})^{n}\phi^{a_{0}}(x).
\end{align*}
By our previous claim the only sensible definition is
\begin{align*}
	\fdv{\lag(\phi^{a})(x)}{\phi^{b}(y)} = \delta_{ab}(\del{}{\mu})^{n}\delta^{d}(x - y).
\end{align*}
This can now be used to compute all sorts of functional derivatives.

\paragraph{Functional Optimization}
Consider a function (or set of functions) $q$ and some functional $S$ of those $q$. Describe the $q$ such that $S$ has an extremum.

What kinds of problems are interesting? Problems containing arbitrarily high derivatives of $q$ are of course important. Problems with $q$ having fixed value at the boundary, different kinds of boundary conditions, no boundary conditions or even with a functional as a boundary condition are also of interest. However, to introduce the involved techniques, we will first be studying problems of the above form.

\paragraph{The Variation of a Function}
We define
\begin{align*}
	\var{q} = \deval{q}{\alpha}{0}\dd{\alpha}..
\end{align*}

\paragraph{The Variation of a Functional}
We now in a similar way define the variation of a functional as
\begin{align*}
	\var{S} = \deval{S}{\alpha}{0}\dd{\alpha}.
\end{align*}

We see that the variation operation behaves exactly like a derivative, and according to the definition, it commutes with all other derivatives that may be involved, assuming sufficient smoothness. These results will therefore be used without further argument.

In these functional derivatives will appear the partial derivative of a function map with respect to a function. What in god's name is that? And what about a derivative with respect to a derivative? And should the derivative with respect to a derivative not contain some information about the derivative of the function the derivative of which we are differentiating with respect to? These are all very good and somewhat complex questions.

Mathematically, it is somewhat beyond me to give a proper answer. But physically, we can think of it the following way: The functions $q^{i}$, as we will see later, will represent the path of the system, and $\tau$ will be replaced with the time $t$. As such, by varying $q^{i}$ we vary the path of the system, and by varying $\dot{q}^{i}$ we vary the velocity with which the system traverses the path. What we are doing with variational calculus is testing out an infinite number of paths to find the extremum of the action, and surely we must both try out different paths and traverse them with different velocities in order to find the extremum. That is why the functions and their derivatives can be treated separately.

What is a derivative with respect to a function? To answer this, recall that at every point $\tau$, the functions are just numbers. When studying the variation of a functional, we are in truth studying its dependence on $\alpha$. In order to do this, the chain rule states that we must first compute an outer partial derivative, which is exactly what the derivative with respect to a function is. This can in turn be indirectly translated to say something about how the functional changes when the function is varied.

\paragraph{The Integral Functional}
The integral functional is the functional
\begin{align*}
	S(q) = \integ{a}{b}{\tau}{F(q, \dot{q})},
\end{align*}
and will be the subject matter of this text from now on.

\paragraph{Solving the Variational Problem}
We have now tried varying the functional around the extremum. Single-variable calculus gives the condition that $\dv{S}{\alpha} = 0$ at the extremum, which is equivalent to $\var{S} = 0$. In other words,
\begin{align*}
	\integ{a}{b}{\tau}{\del{q^{i}}{F}\var{q^{i}} + \del{\dot{q}^{i}}{F}\var{\dot{q}^{i}}} = 0.
\end{align*}
We can integrate this by parts to obtain
\begin{align*}
	\left[\del{\dot{q}^{i}}{F}\var{q^{i}}\right]_{a}^{b} + \integ{a}{b}{\tau}{\left(\del{q^{i}}{F} - \dv{\tau}\del{\dot{q}^{i}}{F}\right)\var{q^{i}}} = 0.
\end{align*}

The first term from the integration by parts can be handled in two ways. If the variational problem has fixed boundary conditions, the families $q^{i}$ must have been chosen such that all functions in the family satisfy the boundary conditions. Thus the variations of these at the endpoints vanish. Otherwise, the two arising terms might be used as boundary conditions themselves (the need for this arises due to the problem in question being second-order, and thus requiring two conditions). As $q^{i}$ may be varied in any way possible, it is thus clear that when using these boundary terms as conditions, they must be set equal to zero.

The remaining integral might of course happen to be zero for the given choice of families of $q^{i}$. But the extremum is an extremum no matter what choice I make. So by changing up the problem - for instance, by reparametrizing the $q^{i}$ or study an entirely different family in a similar way - I still obtain the same results. This must imply that the integral is zero no matter what $\var{q^{i}}$ is. And for this to be true, the only possibility is for the integrand to always be zero. To be absolutely sure, we can try varying only one coordinate at a time. This implies that
\begin{align*}
	\pdv{F}{q^{i}} - \dv{\tau}\pdv{F}{\dot{q}^{i}} = 0
\end{align*}
for all $i$, always. Solutions to this set of equations are thus our extrema, and are called the Euler-Lagrange equations.

\paragraph{Variational Problems With Higher-Order Derivatives}
What if the integrand also involves higher-order derivatives of the $q^{i}$? We can retrace the above steps mostly, but we will have to perform an extra (series of) integration(s) by parts. For instance, by including the second derivative and adding one extra integration, you should be able to show (unless this is wrong) that the extremum solution satisfies
\begin{align*}
	\pdv{F}{q^{i}} - \dv{\tau}\pdv{F}{\dot{q}^{i}} + \dv[2]{\tau}\pdv{F}{\ddot{q}^{i}} = 0.
\end{align*}

\paragraph{The Functional Derivative of the Integral Functional}
For a variational problem with fixed boundary conditions we obtained
\begin{align*}
	\var{S} = \integ{a}{b}{\tau}{\left(\pdv{F}{q^{i}} - \dv{\tau}\pdv{F}{\dot{q}^{i}}\right)\var{q^{i}}}.
\end{align*}
The functional derivative is then
\begin{align*}
	\fdv{S}{q^{i}(t)} = \eval{\pdv{F}{q^{i}} - \dv{\tau}\pdv{F}{\dot{q}^{i}}}_{t}.
\end{align*}
Using this definition, the Euler-Lagrange equations may be written as
\begin{align*}
	\fdv{S}{q^{i}(t)} = 0.
\end{align*}