\section{Group Theory}

\paragraph{Definition of a Group}
A group is a set of objects $G$ with an operation $G\times G\to G,\ (a, b)\to ab$ such that
\begin{itemize}
	\item If $a, b\in G$ then $ab\in G$.
	\item $a(bc) = (ab)c$ for all $a, b, c\in G$.
	\item There exists an identity $e$ such that $ae = ea = a$ for all $a\in G$.
	\item There exists for every element $a$ an inverse $a^{-1}\in G$ such that $aa^{-1} = a^{-1}a = e$.
\end{itemize}
Groups can be
\begin{itemize}
	\item cyclic, i.e. all elements in the group are powers of a single element.
	\item finitie, i.e. groups containing a finite number of elements, or infinte.
	\item discrete, i.e. all elements in the group can be labelled with some index, por continuous.
	\item commutative, i.e. $ab = ba$ for all elements in the group, or non-commutative.
\end{itemize}

\paragraph{Subgroups}
If $G = \{g_{\alpha}\}$ and the subset $H = \{h_{\alpha}\}$ is also a group, we call $H$ a subgroup of $G$ and write $H < G$.

\paragraph{Conjugacy Classes}
Two group elements $a$ and $b$ are conjugate if there exists an element $g$ such that
\begin{align*}
a = gbg^{-1}.
\end{align*}
We write $a\sim b$.

\paragraph{Equivalence Relations}
An equivalence relation is a relation (here denoted $=$) between two things such that
\begin{itemize}
	\item $a = b \equiv b = a$.
	\item $a = b,\ b = c\implies a = c$.
\end{itemize}

\example{Conjugacy as an Equivalence Relation}

\paragraph{Homomorphisms and Isomorphisms}
A homomorphisms is a map $f: G\to H$ such that $f(g_{1})f(g_{2}) = f(g_{1}g_{2})$. If the map is bijective, $f$ is called an isomorphism.

\paragraph{Direct Products}
GIven two groups $F$ and $G$, we define $F\times G$ as the set of ordered pairs of elements of the two groups. The group action of $F\times G$ is the group actions of $F$ and $G$ separately on the elements in the ordered pair.

\paragraph{Generators}
For discrete groups, the generators of a group is the smallest set of elements in the group such that all other elements in the group can be composed by the elements in the set. For continuous groups, we will use the term generators to refer to elements such that any group element can be written as real powers of this element.

\paragraph{Point Groups}
Point groups are symmetries of, for instance, a crystal structure that leave at least one point in the structure invariant. Examples include
\begin{itemize}
	\item rotations.
	\item reflections.
	\item spatial inversions.
\end{itemize}
Combining these with certain discrete translation, you obtain the space groups of the crystal. Space groups are the groups of all symmetries of a crystal.

\paragraph{Dihedral Groups}
The dihedral group $D_{n}$ is the group of transformations that leave an $n$-sided polygon invariant.

\paragraph{Lie Groups}
A Lie group is a group containing a manifold with the group operation and inverse operation being smooth maps. My current understanding of the significance of this is that it allows us to differentiate and expand the group elements with respect to certain parameters.

The group elements are denoted $g(\vb*{\theta})$, where $g(\vb{0}) = 1$. We write them as
\begin{align*}
	g(\vb*{\theta}) = e^{\theta_{a}T_{a}}
\end{align*}
where we have introduced the generators $T_{a}$. This is reasonable, partially because it is reasonable for a smooth operation to have an addition operation (I think), and in this case the series definition of the exponential function yields exactly that any element is generated by powers of the generators. Note however that the generators themselves are typically not group elements, which might leave a hole in this reasoning. In addition, the question of whether the group is Abelian leaves it questionable whether the addition of generators makes sense, but I will return to this concern. There is probably a deeper understanding to this, and what I am saying may even be completely incorrect. Depending on the context, the exponent may also contain a factor $-i$, but this discussion will omit it.

\paragraph{The Lie Algebra}
Expanding an element around the identity yields
\begin{align*}
	g(\vb*{\theta}) \approx 1 + \theta_{a}T_{a}.
\end{align*}
First of all, we note that performing this for exponentials of a single generator yields
\begin{align*}
	e^{\theta_{i}T_{i}}e^{\theta_{j}T_{j}} \approx (1 + \theta_{i}T_{i})(1 + \theta_{j}T_{j}),
\end{align*}
which is equal to $e^{\theta_{i}T_{i} + \theta_{j}T_{j}}$ to first order even for a non-commutative group. Hence the addition of generators is reasonable. Furthermore, this implies that set of generators is a vector space, termed the Lie algebra.

\paragraph{The Lie Bracket}
Having seen that the exponential notation makes sense, we study it for a non-commutative group. The element $e^{-\theta_{i}T_{i}}e^{-\theta_{j}T_{j}}e^{\theta_{i}T_{i}}e^{\theta_{j}T_{j}}$ is equal to the identity for a commutative group, and we would like to study this in the general case. We have
\begin{align*}
	e^{\theta_{i}T_{i}}                    &\approx 1 + \theta_{i}T_{i} + \frac{1}{2}\theta_{i}^{2}T_{i}^{2}, \\
	e^{\theta_{i}T_{i}}e^{\theta_{j}T_{j}} &\approx \left(1 + \theta_{i}T_{i} + \frac{1}{2}\theta_{i}^{2}T_{i}^{2}\right)\left(1 + \theta_{j}T_{j} + \frac{1}{2}\theta_{j}^{2}T_{j}^{2}\right) \approx 1 + \theta_{i}T_{i} + \theta_{j}T_{j} + \frac{1}{2}\left(\theta_{i}^{2}T_{i}^{2} + \theta_{j}^{2}T_{j}^{2}\right) + \theta_{i}\theta_{j}T_{i}T_{j}.
\end{align*}
We thus obtain
\begin{align*}
	e^{-\theta_{i}T_{i}}e^{-\theta_{j}T_{j}}e^{\theta_{i}T_{i}}e^{\theta_{j}T_{j}} \approx& 1 + \theta_{i}T_{i} + \theta_{j}T_{j} + \frac{1}{2}\left(\theta_{i}^{2}T_{i}^{2} + \theta_{j}^{2}T_{j}^{2}\right) + \theta_{i}\theta_{j}T_{i}T_{j} - \theta_{i}T_{i}\left(1 + \theta_{i}T_{i} + \theta_{j}T_{j}\right) \\
	 &- \theta_{j}T_{j}\left(1 + \theta_{i}T_{i} + \theta_{j}T_{j}\right) + \frac{1}{2}\left(\theta_{i}^{2}T_{i}^{2} + \theta_{j}^{2}T_{j}^{2}\right) + \theta_{i}\theta_{j}T_{i}T_{j} \\
	=& 1 + \theta_{i}\theta_{j}T_{i}T_{j} - \theta_{i}\theta_{j}T_{i}T_{j} - \theta_{i}\theta_{j}T_{j}T_{i} + \theta_{i}\theta_{j}T_{i}T_{j} \\
	=& 1 + \theta_{i}\theta_{j}\comm{T_{a}}{T_{b}}
\end{align*}
to second order, where we have introduced the Lie bracket
\begin{align*}
	\comm{T_{i}}{T_{j}} = T_{i}T_{j} - T_{j}T_{i}.
\end{align*}
Hence, the non-commutativity of Lie groups close to the identity and the structure of the Lie algebra is described by the Lie brackets, which is why we study them. Furthermore, we have
\begin{align*}
	e^{-\theta_{i}T_{i}}e^{-\theta_{j}T_{j}}e^{\theta_{i}T_{i}}e^{\theta_{j}T_{j}} \approx e^{\theta_{i}\theta_{j}\comm{T_{a}}{T_{b}}},
\end{align*}
implying that the Lie bracket belongs to the vector space spanned by the generators and allowing us to write
\begin{align*}
	\comm{T_{a}}{T_{b}} = f_{a, b, c}T_{c}.
\end{align*}
The constants $f_{a, b, c}$ are called structure constants.

\example{Rotations in Two Dimensions}
Consider a rotation of an infinitesimal displacement $\dd{\vb{x}}$ with a rotation $R$. The requirement for length to be preserved implies $R^{T}R = 1$.

Consider now a rotation by a small angle $\var{\theta}$. Taylor expanding it in terms of the angle yields
\begin{align*}
	R(\var{\theta}) \approx 1 + A\var{\theta}.
\end{align*}
The requirement for $R$ to be orthogonal yields $A^{T} = -A$. We choose the solution
\begin{align*}
	J =
	\mqty[
		0  & 1 \\
		-1 & 0
	].
\end{align*}
We can now write the rotation matrix as
\begin{align*}
	R(\var{\theta}) =
	\mqty[
		1             & \var{\theta} \\
		-\var{\theta} & 1
	].
\end{align*}

We would now like to construct a large rotation in terms of smaller rotations as
\begin{align*}
	R(\theta) = \lim\limits_{N\to\infty}\left(1 + \frac{\theta}{N}J\right)^{N} = e^{\theta J}.
\end{align*}
We can write this as an infinite series and use the fact that $J^{2} = -1$ to obtain
\begin{align*}
	R(\theta) = \cos{\theta} + J\sin{\theta}.
\end{align*}

\example{Rotations in Three Dimensions}
The argument done for two dimensions does not use the dimensionality, so we conclude that even for higher dimensions, $R^{T}R = 1$. Expanding a small rotation around the identity yields that the first-order term must include an antisymmetric matrix. The space of antisymmetric $3\times 3$ matrices is three-dimensional. We thus choose the basis
\begin{align*}
	J_{x} =
	\mqty[
		0 & 0  & 0 \\
		0 & 0  & 1 \\
		0 & -1 & 0
	],
	J_{y} =
	\mqty[
		0 & 0 & -1 \\
		0 & 0 & 0  \\
		1 & 0 & 0
	],
	J_{z} =
	\mqty[
		0  & 1 & 0 \\
		-1 & 0 & 0 \\
		0  & 0 & 0
	].
\end{align*}
Exponentiating yields
\begin{align*}
	R(\theta) = e^{\sum \theta_{i}J_{i}} = e^{\vb{\theta}\cdot\vb{J}}.
\end{align*}
In physics we usually extract a factor $i$ such that the basis matrices are Hermitian, and the rotation becomes
\begin{align*}
	R(\theta) = e^{i\vb{\theta}\cdot\vb{J}}.
\end{align*}

The set of generators of these rotations constitutes the Lie algebra.

We know in general that rotations in three dimensions do not commute. In fact, we obtain in general that
\begin{align*}
	R(\vb{\theta})R(\vb{\theta}')R^{-1}(\vb{\theta}) = \theta_{a}\theta_{b}'\comm{J_{a}}{J_{b}},
\end{align*}
where $\comm{J_{a}}{J_{b}}$ is the commutator. This commutator satisfies
\begin{align*}
	\comm{J_{a}}{J_{b}}^{T} = \comm{J_{b}^{T}}{J_{a}^{T}} = \comm{-J_{b}}{-J_{a}} = -\comm{J_{a}}{J_{b}},
\end{align*}
which implies
\begin{align*}
	\comm{J_{a}}{J_{b}} = f_{a,b,c}J_{c}.
\end{align*}
It can be shown that
\begin{align*}
	\comm{J_{i}}{J_{j}} = \varepsilon_{i,j,k}J_{k},
\end{align*}
or in a physics context (where a factor $i$ is extracted):
\begin{align*}
	\comm{J_{i}}{J_{j}} = i\varepsilon_{i,j,k}J_{k}.
\end{align*}

\paragraph{Representations}
A representation of a group on a vector space is a homomorphism $D: G\to\GL{V}$, where $\GL{V}$ is the group of all invertible linear transformations on $V$. The dimension of the representation is defined as the dimension of $V$.

\paragraph{Representations on Direct Product Spaces}
Suppose that there exists representations $\rho_{V}$ and $\rho_{W}$ of $G$ on $V$ and $W$ respectively. The representation $\rho_{V\oplus W}$ of $G$ on $V\oplus W$ is then defined by
\begin{align*}
	\rho_{V\oplus W}(a)(\vb{v}, \vb{w}) = (\rho_{V}(a)(\vb{v}), \rho_{W}(a)(\vb{w})).
\end{align*}

\paragraph{Representations on Tensor Product Spaces}
Suppose that there exists representations $\rho_{V}$ and $\rho_{W}$ of $G$ on $V$ and $W$ respectively. The representation $\rho_{V\otimes W}$ of $G$ on $V\otimes W$ is then defined by
\begin{align*}
	\rho_{V\otimes W}(a)(\vb{v}\otimes\vb{w}) = (\rho_{V}(a)(\vb{v}))\otimes(\rho_{W}(a)(\vb{w})).
\end{align*}

\paragraph{The Unitarity Theorem}
Finite groups have unitary representations - with some inner product, at least. Such representations will be the primary concern of the following discussion.

\paragraph{Reducible Representations}
Two representations are equivalent if they satisfy $U\rho(a)U^{-1} = \rho^{\prime}(a)$ for any group element $a$ and some linear operator $U$. $\rho$ is reducible if it is equivalent to a representation of the form
\begin{align*}
	\mqty[
		\rho_{1} & \sigma \\
		0        & \rho_{2}
	],
\end{align*}
where $\rho_{1}$ and $\rho_{2}$ are representations of dimension $n_{1}$ and $n_{2}$ respectively and $\sigma$ is an $n_{1}\times n_{2}$ block satisfying $\sigma(ab) = \rho_{1}(a)\sigma(b) + \sigma(a)\rho_{1}(b)$. A basis can often be chosen such that $\sigma = 0$.

\example{$\SO{3}$}
As we know, \SO{3} may be represented by rotations in three dimensions. In particular, we may choose a basis for $\R^{3}$ such that
\begin{align*}
	\rho(R_{\vb{e}_{z}}^{3}) = 
	\mqty[
		\cos(\theta) & -\sin(\theta) & 0 \\
		\sin(\theta) & \cos(\theta)  & 0 \\
		0            & 0             & 1
	].
\end{align*}
The upper left block is a representation of \SO{2}, and the lower right block is the trivial representation $\rho(a) = 1$. Hence $\rho$ is reducible.

Note that the upper right block is zero. For finite-dimensional groups, one can apparently always choose a basis such that this is possible. In this case we may write $\rho(R_{\vb{e}_{z}}^{3}) = \rho_{2}\oplus 1$, where $\rho_{2}$ is the representation of \SO{2}.

\example{Tensor Product Representations}
Given some representation $\rho$ on $V$, we consider the representation $\rho_{V\otimes V} = \rho\otimes\rho$. Next, we split a tensor $T$ into its symmetric components $T_{\{ij\}}$ and antisymmetric components $T_[ij]$ such that $T_{ij} = T_{\{ij\}} + T_[ij]$. Next, we have for a symmetric tensor $S$
\begin{align*}
	(\rho\otimes\rho S)_{ij} = \rho_{ik}\rho_{jl}S_{kl} = \rho_{jl}\rho_{ik}S_{lk} = (\rho\otimes\rho S)_{ji},
\end{align*}
i.e. the representation preserves the symmetry of $S$. The same holds for antisymmetric tensors. Denoting the symmetric and antisymmetric subspaces as $(V\otimes V)_{\pm}$ we have $(V\otimes V)_{+}\oplus(V\otimes V)_{-}$, we thus have
\begin{align*}
	\rho\otimes\rho =
	\mqty[
		\rho_{(V\otimes V)_{+}} & 0 \\
		0                       & \rho_{(V\otimes V)_{-}}
	],
\end{align*}
and $\rho\otimes\rho$ has been reduced to $\rho_{(V\otimes V)_{+}}\oplus\rho_{(V\otimes V)_{-}}$, which is the direct sum of its symmetric and antisymmetric representation.

\paragraph{Irreducible Representations}
If a representation cannot be reduced, it is irreducible. Such representations are often called irreps.

By their very nature, all representations leave at least some part of the vector space invariant - all of it, in the worst case. Suppose now that there is a part of the vector space that is left invariant by the representation and another that is not. In this case, the representation would necessarily take the above block diagonal form, meaning that it can be reduced. The consequence of this is that for any non-zero $v\in V$, the fact that $\rho$ is an irrep implies that $V$ is spanned by elements of the form $\rho(a)v$.

\paragraph{Schur's Lemma}
Schur's lemma has two statements:
\begin{itemize}
	\item Given an irrep $\rho$ on $V$, any operator $A$ that commutes with $\rho$ is a multiple of the identity on $V$.
	\item Given two inequivalent irreps $\rho_{1}$ and $\rho_{2}$ on vector spaces $V_{1}$ and $V_{2}$, the only linear transformation $B$ such that $B\rho_{1} = \rho_{2}B$ is the zero transformations.
\end{itemize}

To prove the first, consider an eigenvector $v$ of $A$ with eigenvalue $\lambda$, the existence of which is guaranteed by the fundamental theorem of algebra. Assuming $A$ to commute with $\rho$, we have
\begin{align*}
	A\rho(a)v = \rho(a)Av = \lambda\rho(a)v,
\end{align*}
hence $\rho$ leaves eigenspaces of $A$Â invariant. As $V$ is spanned by elements of the form $\rho(a)v$, we conclude that $A = \lambda$ on all of $V$.

To prove the second, assume the two representations to satisfy the above and let the $V_{i}$ having dimensionalities $n_{i}$. The first case is $n_{2} < n_{1}$. The kernel of $B$ is then non-trivial, and for any vector $v$ in it we find
\begin{align*}
	B\rho_{1}v = \rho_{2}Bv = 0,
\end{align*}
hence $\rho_{1}$ leaves the kernel of $B$ invariant. This implies, however, that all of $V_{1}$ must be the kernel of $B$ and $B = 0$ in this case. The second case is $n_{2} \geq n_{1}$, for which we study vectors $w = Bv$. We find
\begin{align*}
	\rho_{2}w = \rho_{2}Bv = B\rho_{1}v,
\end{align*}
hence $\rho_{2}$ preserves the image of $B$. The possible consequences of this are either that $B = 0$ or that the image of $B$ is all of $V_{2}$. The latter is impossible if $n_{2} > n_{1}$, meaning that $B = 0$ in this case. Finally, if $n_{1} = n_{2}$ and the image of $B$ is all of $V_{2}$, that would imply that $B$ is bijective, This would, however, imply that $\rho_{1}$ and $\rho_{2}$ are equivalent, which was assumed not to be the case, hence $B = 0$ for this case too.

\paragraph{The Orthogonality Theorem}
For a group $G$ we can number the possible inequivalent irreps $\rho_{\mu}$, as well as the vector spaces and corresponding dimensionalities $V_{\mu}$ and $n_{\mu}$. Given an operator $T: V_{\nu}\to V_{\mu}$ we may then define the operator
\begin{align*}
	B = \sum\limits_{a}\rho_{\mu}(a)T\rho_{\nu}(a^{-1}).
\end{align*}
We then find
\begin{align*}
	\rho_{\mu}(b)B &= \rho_{\mu}(b)\sum\limits_{a}\rho_{\mu}(a)T\rho_{\nu}(a^{-1}) \\
	               &= \sum\limits_{a}\rho_{\mu}(ba)T\rho_{\nu}(a^{-1}) \\
	               &= \sum\limits_{c}\rho_{\mu}(c)T\rho_{\nu}(cb^{-1}) \\
	               &= \sum\limits_{c}\rho_{\mu}(c)T\rho_{\nu}(c)\rho_{\nu}(b^{-1}) \\
	               &= B\rho_{\nu}(b^{-1}).
\end{align*}
Schur's lemma implies that $B = \lambda_{\mu, T}$ (where we have labelled the eigenvalue with the necessary information) if $\mu = \nu$ and $B = 0$ otherwise. We may summarize this as
\begin{align*}
	B = \lambda_{\mu, T}\delta_{\mu\nu},
\end{align*}
where we forego Einstein notation for the rest of this group theory discussion.

In particular, we consider the operator such that $T_{ij} = \delta_{ip}\delta_{jq}$ for some particular $p, q$. Relabelling the eigenvalue as $\lambda_{\mu, pq}$ we find
\begin{align*}
	\sum\limits_{k, l}\sum\limits_{a}\rho_{\mu, ik}(a)T_{kl}\rho_{\nu, lj}(a^{-1}) = \sum\limits_{k, l}\sum\limits_{a}\rho_{\mu, ik}(a)\delta_{kp}\delta_{lq}\rho_{\nu, lj}(a^{-1}) = \sum\limits_{a}\rho_{\mu, ip}(a)\rho_{\nu, qj}(a^{-1}) = \lambda_{pq, \mu}\delta_{\mu\nu}\delta_{ij}.
\end{align*}
We will try to simplify this by computing the eigenvalue. In order for the above to be non-trivial, we must set $\mu = \nu$. Computing the trace of either side we find
\begin{align*}
	\lambda_{pq, \mu}n_{\mu} = \sum\limits_{i}\sum\limits_{a}\rho_{\mu, ip}(a)\rho_{\mu, qi}(a^{-1}) = \sum\limits_{a}(\rho_{\mu}(a^{-1})\rho_{\mu}(a))_{qp} = \sum\limits_{a}(\rho_{\mu}(e))_{qp} = \sum\limits_{a}\delta_{qp} = \delta_{qp}N_{G},
\end{align*}
where $N_{G}$ is the order of the group. Thus we have the orthogonality relation
\begin{align*}
	\sum\limits_{a}\rho_{\mu, ip}(a)\rho_{\nu, qj}(a^{-1}) = \frac{N_{G}}{n_{\mu}}\delta_{qp}\delta_{\mu\nu}\delta_{ij}.
\end{align*}

\paragraph{The Character}
We will need to classify representations in terms of something that is basis-independent. One such property is the trace, as if $\rho_{1}$ and $\rho_{2}$ are equivalent we have
\begin{align*}
	\tr(\rho_{2}) = \tr(U\rho_{2}U^{-1}) = \tr(U^{-1}U\rho_{2}) = \tr(\rho_{2}).
\end{align*}
Hence we define the character
\begin{align*}
	\chi_{\rho}(a) = \tr(\rho(a)),
\end{align*}
which is the same for all equivalent representations.

\paragraph{Properties of the Character}
We have
\begin{align*}
	\chi_{\rho}(e) = n_{\rho},
\end{align*}
where $n_{\rho}$ is the dimension of the representation. Next, if $a$ and $b$ belong to the same conjugacy class we have
\begin{align*}
	\chi_{\rho}(a) = \chi_{\rho}(gbg^{-1}) = \tr(\rho(gbg^{-1})) = \tr(\rho(g)\rho(b)\rho(g^{-1})) = \chi_{\rho}(b).
\end{align*}
Finally, if
\begin{align*}
	\rho = \bigoplus\limits_{\mu}\rho_{\mu}
\end{align*}
we have
\begin{align*}
	\chi_{\rho} = \sum\limits_{\mu}\chi_{\rho_{\mu}}.
\end{align*}

\paragraph{The Character inner Product}
We define the character inner product
\begin{align*}
	\braket{\chi_{\mu}}{\chi_{\nu}} = \frac{1}{N_{G}}\sum\limits_{a}\chi_{\mu}(a)\chi_{\nu}(a^{-1}) = \frac{1}{N_{G}}\sum\limits_{a}\chi_{\mu}(a)\cc{\chi_{\nu}(a)},
\end{align*}
where the last step follows from assuming unitarity. it may also be computed as
\begin{align*}
	\braket{\chi_{\mu}}{\chi_{\nu}} = \frac{1}{N_{G}}\sum\limits_{C_{i}}k_{i}\chi_{\mu}(C_{i})\cc{\chi_{\nu}(C_{i})},
\end{align*}
where the $C_{i}$ are the different conjugacy classes and $k_{i}$ their order.

\paragraph{Orthogonality of Characters}
Starting from the relation we may consider element $pq$ and sum over $p$ and $q$ to find
\begin{align*}
	\sum\limits_{p, q}\sum\limits_{a}\rho_{\mu, pp}(a)\rho_{\nu, qq}(a^{-1}) = \sum\limits_{a}\chi_{\mu}(a)\chi_{\nu}(a^{-1}) = N_{G}\delta_{\mu\nu},
\end{align*}
where we have introduced the shorthand $\chi_{\mu} = \chi_{\rho_{\mu}}$. We can now use the character inner product to arrive at the orthogonality relation
\begin{align*}
	\braket{\chi_{\mu}}{\chi_{\nu}} = \delta_{\mu\nu}.
\end{align*}

\paragraph{Decomposition of Representations}
Any representations may be written as a direct sum over irreps according to
\begin{align*}
	\rho = \bigoplus\limits_{k}\rho_{\mu_{k}},
\end{align*}
where the $\rho_{\mu_{k}}$ are the different irreps. Alternatively we may sum over the various types of irreps according to
\begin{align*}
	\rho = \bigoplus\limits_{\mu}k_{\mu}\rho_{\mu},
\end{align*}
where the $k_{\mu}$ are the numbers of times each individual irrep occurs. The character of this representation is
\begin{align*}
	\chi_{\rho} = \sum\limits_{\mu}k_{\mu}\chi_{\mu}.
\end{align*}
Projection onto a particular $\chi_{\nu}$ yields
\begin{align*}
	\braket{\chi_{\rho}}{\chi_{\mu}} = \sum\limits_{\mu}k_{\mu}\braket{\chi_{\mu}}{\chi_{\nu}} = \sum\limits_{\mu}k_{\mu}\delta_{\mu\nu} = k_{\nu}.
\end{align*}
This implies that the decomposition may be specified by computing the inner products $\braket{\chi_{\rho}}{\chi_{\mu}}$.