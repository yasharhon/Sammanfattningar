\section{Basic Concepts in Statistical Physics}

\paragraph{Avogadro's Number}
Statistical physics discusses systems of many particles. A relevant measure of the number of particles to be studies is $N_{\text{A}} = \num{6.022e23}$.

\paragraph{Molar Mass}
The molar mass of a substance is defined as $M = mN_{\text{A}}$, where $m$ is the mass of a single atom or molecule.

\paragraph{Atomic Units}
When discussing atoms and molecules, we use relative units. These units are relative to the atomic mass unit $\au = \SI{1.66e-27}{\kilo\gram}$, defined as $\frac{1}{12}$ the mass of \ce{^{12}C}. This happens to be close to the mass of a hydrogen atom.

\paragraph{The Thermodynamic Limit}
The thermodynamic limit is the limit of the statistical consideration of a system when the number of particle is large. In this limit, quantities such as temperature, pressure and density can be defined as we know them and macroscopic equilibria can be achieved.

\paragraph{Microstates}
A microstate of a system is any complete description of all particles in a system, for instance a specification of all positions and velocities of the particles in a gas.

\paragraph{Macrostates}
A macrostate of a system is a description of the macroscopic properties of a system.

\paragraph{Multiplicity}
The multiplicity of a macrostate is the number of microstates that yield the same macrostate.

\paragraph{The Fundamental Postulate}
The fundamental postulate of statistical mechanics is that all microstates available to a system are observed with equal probability.

\paragraph{Equilibrium and Multiplicity}
Combining the fundamental postulate with our knowledge of thermodynamics, it is clear that a system in thermal equilibrium is in the macrostate corresponding to maximal multiplicity.

\paragraph{The Boltzmann Constant and Microscopic Temperature}
Consider two isolated systems in contact. The total energy and multiplicity is given by
\begin{align*}
	E = E_{1} + E_{2},\ \Omega = \Omega_{1}(E_{1})\Omega_{2}(E_{2}).
\end{align*}
At equilibrium, the total multiplicity is maximal. Differentiating with respect to $E_{1}$ gives
\begin{align*}
	\pdv{\Omega}{E_{1}}                        &= \Omega_{2}\dv{\Omega_{1}}{E_{1}} + \Omega_{1}\dv{\Omega_{2}}{E_{2}}\dv{E_{2}}{E_{1}} = \Omega_{2}\dv{\Omega_{1}}{E_{1}} - \Omega_{1}\dv{\Omega_{2}}{E_{2}}, \\
	\frac{1}{\Omega_{1}}\dv{\Omega_{1}}{E_{1}} &= \frac{1}{\Omega_{2}}\dv{\Omega_{2}}{E_{2}},
\end{align*}
which we can rewrite as
\begin{align*}
	\dv{\ln{\Omega_{1}}}{E_{1}} = \dv{\ln{\Omega_{2}}}{E_{2}}.
\end{align*}
We define either side to be equal to $\frac{1}{\kb T}$.

\paragraph{The Boltzmann Factor}
Consider a thermal bath in contact with a small system. The energy of the small system is $\varepsilon$, hence the bath has energy $E - \varepsilon$. Assuming that the multiplicity of the small system is independent of its energy, we Taylor expand the total multiplicity to obtain
\begin{align*}
	\ln{\Omega(E - \varepsilon)} \approx \ln{\Omega(E)} - \varepsilon\dv{\ln{\Omega}}{E} = \ln{\Omega(E)} - \frac{\varepsilon}{\kb T},
\end{align*}
with solution
\begin{align*}
	\Omega(E - \varepsilon) = \Omega(E)e^{-\frac{\varepsilon}{\kb T}}.
\end{align*}
This exponential factor is called the Boltzmann factor.

A consequence of this is that the probability of finding the small system in the macrostate with energy $\varepsilon$ is, according to the fundamental hypothesis, proportional to $e^{-\frac{\varepsilon}{\kb T}}$.

\paragraph{The Statistical Basis For Entropy}
We have
\begin{align*}
	\frac{1}{T} = \fix{\pdv{S}{U}}{V}.
\end{align*}
Combining this with our definition of temperature from a microscopic perspective, we obtain
\begin{align*}
	S = \kb\ln{\Omega}.
\end{align*}

Another, slightly more ad hoc way of arguing this, is to consider the entropy as a function of the multiplicity. Supposing that we have a system comprised of two non-correlated parts, the extensivity of the entropy yields
\begin{align*}
	S = S_{1} + S_{2}.
\end{align*}
Now, the total multiplicity is $\Omega = \Omega_{1}\Omega_{2}$, so requiring the entropy to be extensive takes the form
\begin{align*}
	S(\Omega_{1}\Omega_{2}) = S(\Omega_{1}) + S(\Omega_{2}).
\end{align*}
A simple choice of entropy function is thus to choose one among the elementary functions. The function which does this is the natural logarithm. At this point we may introduce the Boltzmann constant as the constant of proportionality required to give $S$ a dimension, and thus start our statistical definitions with our new entropy formula. The statistical definition of entropy will follow from that.

\paragraph{The Gibbs Factor}
Consider a system of $N$ particles with internal energy $U$ exchanging energy and particles with a smaller system with energy $\varepsilon$ and $n$ particles. The entropy of the reservoir is
\begin{align*}
	S \approx S_{0} - \varepsilon\fix{\pdv{S}{U}}{N, V} - n\fix{\pdv{S}{N}}{U, V} = S_{0} - \frac{1}{T}\left(\varepsilon - \mu n\right).
\end{align*}
Writing this in terms of the multiplicity we have
\begin{align*}
	\ln{\Omega(E - \varepsilon, N - n)} &= \ln{\Omega(E, N)} - \frac{\varepsilon - \mu n}{\kb T}, \\
	\Omega(E - \varepsilon, N - n)      &= \Omega(E, N)e^{-\frac{\varepsilon - \mu n}{\kb T}},
\end{align*}
implying
\begin{align*}
	P(\varepsilon, n) \propto e^{-\beta\left(\varepsilon - \mu n\right)}.
\end{align*}

\paragraph{Entropy and Probability}
Consider a system with $N$ available microstates, all equally likely and indistinguishable by experiment. These are divided into groups called to macrostates, distinguishable by experiment, with $n_{i}$ microstates in each macrostate. We have
\begin{align*}
	\sum n_{i} = N,
\end{align*}
and the probability of finding the system in a particular macrostate is thus
\begin{align*}
	P_{i} = \frac{n_{i}}{N}.
\end{align*}
The total entropy is $S_{\text{tot}} = \kb\ln{N}$, but cannot be measured as the microstates are indistinguishable. Nevertheless, as entropy is extensive, the total entropy is a sum of the freedom to explore the microstates for a given macrostate and the freedom to explore the different macrostates, i.e.
\begin{align*}
	S_{\text{tot}} = S + S_{\text{micro}}.
\end{align*}
The entropy due to exploring microstates is an expectation value over the different macrostates of the entropy, given by
\begin{align*}
	S_{\text{micro}} = \sum P_{i}S_{i} = \kb\sum P_{i}\ln{n_{i}}.
\end{align*}
This yields
\begin{align*}
	S &= S_{\text{tot}} - S_{\text{micro}} \\
	  &= \kb\ln{N} - \kb\sum P_{i}\ln{n_{i}} \\
	  &= \kb\sum P_{i}\left(\ln{N} - \ln{n_{i}}\right) \\
	  &= -\kb\sum P_{i}\ln{P_{i}}.
\end{align*}